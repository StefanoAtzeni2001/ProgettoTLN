{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzioni di Gestione files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "from conllu import parse, TokenList # type: ignore\n",
    "\n",
    "#deserialize data from a file\n",
    "def load_data(file_name,language):\n",
    "    path=\"../data/\"+language+\"/\"+file_name\n",
    "    try: \n",
    "        file = open(path, 'rb') \n",
    "        data = pickle.load(file) \n",
    "        return data\n",
    "    except: \n",
    "        print(\"Error in reading data\")\n",
    "\n",
    "# function to read data from file\n",
    "def read_dataset(file_name,language):\n",
    "    path=\"../data/\"+language+\"/dataset/\"+file_name+\".conllu\"\n",
    "    data = pd.read_csv (path, sep = '\\t',quoting=3, names=[\"POSITION\",\"WORD\",\"TAG\"])\n",
    "    return data\n",
    "\n",
    "# Funzione per salvare il DataFrame in un file CoNLL-U\n",
    "def save_to_conllu(dataframe,file_name,language):\n",
    "    # Creazione della lista di token da DataFrame\n",
    "    path=\"../data/\"+language+\"/tagging/\"+file_name\n",
    "    tokens = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        token = {\n",
    "            \"id\": row['POSITION'],\n",
    "            \"form\": row['WORD'],\n",
    "            \"misc\":  row['TAG']\n",
    "        }\n",
    "        tokens.append(token)\n",
    "    \n",
    "    # Creazione dell'oggetto TokenList\n",
    "    token_list = TokenList(tokens)\n",
    "    \n",
    "    # Scrittura del TokenList nel file CoNLL-U\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(token_list.serialize())\n",
    "    print(\"DataFrame salvato in formato CoNLL-U:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzioni di manipolazione e creazione del golden system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_dataframe(df):\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        position = row['POSITION']\n",
    "        if position == 0 and current_sentence:  # Se è il primo elemento di una nuova frase e c'è una frase in corso\n",
    "            sentences.append(' '.join([str(word) for word in current_sentence]))  # Aggiungi la frase corrente alla lista delle frasi\n",
    "            current_sentence = []  # Inizia una nuova frase\n",
    "        \n",
    "        current_sentence.append(row['WORD'])  # Aggiungi la parola corrente alla frase in corso\n",
    "\n",
    "    # Aggiungi l'ultima frase alla lista delle frasi se presente\n",
    "    if current_sentence:\n",
    "        sentences.append(' '.join([word for word in current_sentence]))\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementazione dell'algoritmo di viterbi con la prevenzione dell'underflow tramite logaritmo e probabilità iniziale omogenea\n",
    "\n",
    "def viterbi(emission_df, transition_df):\n",
    "    # Numero di stati\n",
    "    num_states = len(transition_df)\n",
    "\n",
    "    # Inizializzazione della matrice di probabilità\n",
    "    dp = pd.DataFrame(index=range(num_states), columns=range(len(emission_df.columns)))\n",
    "    pi = 1 / num_states #Probabilità iniziale equiprobabile\n",
    "    dp.iloc[:, 0] = np.log(pi) + np.log(emission_df.iloc[:, 0] + 1e-10)\n",
    "\n",
    "    # Inizializzazione del percorso ottimale\n",
    "    path = {state: [state] for state in range(num_states)}\n",
    "\n",
    "    # Ciclo attraverso le osservazioni\n",
    "    for t in range(1, len(emission_df.columns)):\n",
    "        new_path = {}\n",
    "\n",
    "        # Ciclo attraverso i possibili stati\n",
    "        for state in range(num_states):\n",
    "            # Calcolo della probabilità massima\n",
    "            max_prob = float('-inf')\n",
    "            max_state = None\n",
    "            for prev_state in range(num_states):\n",
    "                prob = dp.iloc[prev_state, t-1] + np.log(transition_df.iloc[state, prev_state] + 1e-10) + np.log(emission_df.iloc[state, t] + 1e-10)\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_state = prev_state\n",
    "            \n",
    "            dp.iloc[state, t] = max_prob\n",
    "\n",
    "            # Aggiornamento del percorso ottimale\n",
    "            new_path[state] = path[max_state] + [state]\n",
    "\n",
    "        path = new_path\n",
    "\n",
    "    # Ritorno del percorso ottimale\n",
    "    max_prob = dp.iloc[:, len(emission_df.columns)-1].max()\n",
    "    max_path = path[dp.iloc[:, len(emission_df.columns)-1].idxmax()]\n",
    "\n",
    "    # Stampa a schermo il percorso di Viterbi\n",
    "    # print('Il percorso di Viterbi è:', ' -> '.join(emission_df.index[max_path]))\n",
    "\n",
    "    return pd.DataFrame({'POSITION': range(len(max_path)), 'WORD': emission_df.columns, 'TAG': [emission_df.index[state] for state in max_path]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione del sub-dataset di probabilità di emissione per le parole di una frase.\n",
    "\n",
    "Applicazione di diverse tecniche di smoothing per gestire le parole sconosciute:\n",
    "\n",
    "1 - Sempre O: P(unk|O) = 1\n",
    "\n",
    "2 - Sempre O o MISC: P(unk|O)=P(unk|B-MISC)=0.5\n",
    "\n",
    "3 - Uniforme: P(unk|tag) = 1/#(NER_TAGs)\n",
    "\n",
    "4 - Statistica TAG sul val set: parole che compaiono 1 sola volta  -> unknown_prob calcolata nel file learning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prende in input una lista di frasi, le probabilità di emisione e transizione apprese \n",
    "#restituisce le coppie parola-NER_TAG assegnate utilizzano l'algoritmo di Viterbi e applicando la tecnica di smoothing specificata\n",
    "def viterbi_tagger(sentences, emission_prob, transition_prob, unkown_prob, smoothing_type):\n",
    "    #inizializzazione\n",
    "    tags=transition_prob.keys()\n",
    "    transition_df = pd.DataFrame.from_dict(transition_prob)\n",
    "    all_sentences_tag = pd.DataFrame()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        emission_sentence_df = pd.DataFrame(columns=words,index=tags)\n",
    "     \n",
    "        for word in words:\n",
    "            if word in emission_prob:\n",
    "                emission_sentence_df[word] = pd.Series(emission_prob[word]).values\n",
    "            else: #applicazione dello smoothing\n",
    "                if (smoothing_type==1): emission_sentence_df[word] =  {tag: 1 if tag == \"O\" else 0 for tag in tags}\n",
    "                elif (smoothing_type==2): emission_sentence_df[word] =  {tag: 0.5 if tag == \"B-MISC\" or tag == \"O\" else 0.0001 for tag in tags}\n",
    "                elif (smoothing_type==3): emission_sentence_df[word] =  {tag: 1/len(tags) for tag in tags}\n",
    "                elif (smoothing_type==4): emission_sentence_df[word] =  unkown_prob\n",
    "\n",
    "        sent_tag=viterbi(emission_sentence_df, transition_df)\n",
    "        all_sentences_tag=pd.concat([all_sentences_tag, sent_tag], axis=0)\n",
    "    return all_sentences_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVE TAGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive tagger --> utilizza la probabilità di emissione più alta, se parola sconosciuta --> B-MISC\n",
    "\n",
    "def naive_tagger(sentences, emission_prob):\n",
    " tags = []\n",
    " final_df = pd.DataFrame()\n",
    "\n",
    " for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    tags = []\n",
    "    for word in words:\n",
    "        if word in emission_prob:\n",
    "            tags.append(max(emission_prob[word], key=emission_prob[word].get))\n",
    "        else:\n",
    "            tags.append(\"B-MISC\")\n",
    "    \n",
    "    # Creazione del DataFrame\n",
    "    df = pd.DataFrame({'WORD': words, 'TAG': tags})\n",
    "    df['POSITION'] = df.index  # Aggiunge la colonna POSITION\n",
    "    df = df[['POSITION', 'WORD', 'TAG']]  # Riordina le colonne\n",
    "\n",
    "    final_df=pd.concat([final_df, df], axis=0)  \n",
    " return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esempio di Decoding Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inizio lingua: en\n",
      "DataFrame salvato in formato CoNLL-U: ../data/en/tagging/golden_tag.conllu\n",
      "DataFrame salvato in formato CoNLL-U: ../data/en/tagging/viterbi_tag.conllu\n",
      "DataFrame salvato in formato CoNLL-U: ../data/en/tagging/naive_tag.conllu\n",
      "fine lingua: en\n",
      "inizio lingua: it\n",
      "DataFrame salvato in formato CoNLL-U: ../data/it/tagging/golden_tag.conllu\n",
      "DataFrame salvato in formato CoNLL-U: ../data/it/tagging/viterbi_tag.conllu\n",
      "DataFrame salvato in formato CoNLL-U: ../data/it/tagging/naive_tag.conllu\n",
      "fine lingua: it\n",
      "inizio lingua: es\n",
      "DataFrame salvato in formato CoNLL-U: ../data/es/tagging/golden_tag.conllu\n",
      "DataFrame salvato in formato CoNLL-U: ../data/es/tagging/viterbi_tag.conllu\n",
      "DataFrame salvato in formato CoNLL-U: ../data/es/tagging/naive_tag.conllu\n",
      "fine lingua: es\n"
     ]
    }
   ],
   "source": [
    "smoothing_type=3\n",
    "num_sentences=1000\n",
    "for language in [\"en\",\"it\",\"es\"]:\n",
    " print( \"inizio lingua: \"+language) \n",
    " emission_prob=load_data(\"emission_prob\",language)\n",
    " transition_prob=load_data(\"transition_prob\",language)\n",
    " unkown_prob=load_data(\"unknown_prob\",language)\n",
    "\n",
    " #carico il file di test, estraggo la sentence e pongo il test set df come golden_df\n",
    " golden_tot = read_dataset(\"test\",language)\n",
    " all_sentence = extract_sentences_from_dataframe(golden_tot)\n",
    " sentences=all_sentence[:num_sentences]\n",
    " vit_df=viterbi_tagger(sentences,emission_prob,transition_prob,unkown_prob,smoothing_type=smoothing_type)\n",
    " naive_df=naive_tagger(sentences, emission_prob)\n",
    "\n",
    " save_to_conllu(golden_tot, \"golden_tag.conllu\", language)\n",
    " save_to_conllu(vit_df, \"viterbi_tag.conllu\", language)\n",
    " save_to_conllu(naive_df, \"naive_tag.conllu\", language)\n",
    " open('../data/'+language+'/tagging/smoothing_type', 'w').write(str(smoothing_type))\n",
    " print( \"fine lingua: \"+language)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TLNenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
