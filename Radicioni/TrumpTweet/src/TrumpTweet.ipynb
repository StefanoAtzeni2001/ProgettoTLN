{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweetting like a Trump\n",
    "\n",
    "### Learning\n",
    "\n",
    "Questo codice utilizza un dataset dei tweet di Donald Trump per generare nuovi tweet utilizzando gli n-grammi.\\\n",
    "Il dataset originale è stato leggermente pulito da alcuni caratteri speciali con bassa frequenza. tweet_clean.csv\\\n",
    "Vengono ottenuti gli n-grammi dalle frasi; un n-gramma consiste in n parole consecutive, strutturate come $((w_{1}, ..., w_{n-1}), w_{n})$. \\\n",
    "In questo modo, abbiamo già la divisione tra **contesto** e **parola da predire**.\\\n",
    "Vengono aggiunti token speciali come padding per indicare l'inizio `<S>` e la fine `</S>`  di una frase.\\\n",
    "Le probabilità di transizione vengono calcolate per ogni parola dato il suo contesto, contando le occorrenze di ogni n-gramma e di ogni contesto.\n",
    "\n",
    "##### Probabilità n-gramma: $p(w_i|w_{i-1}...w_{i-(n-1)})= \\frac{C(w_{i-(n-1)}...w_{i-1},w_i)}{C(w_{i-1}...w_{i-(n-1)})}$  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Probabilità 2-gramma: $p(w_i|w_{i-1})= \\frac{C(w_{i-1},w_i)}{C(w_{i-1})}$\n",
    "\n",
    "\n",
    "### Decoding\n",
    "Una frase viene generata scegliendo una parola alla volta dato il contesto precedente, fino alla generazione di `</S>`  o al raggiungimento di una lunghezza massima.\\\n",
    "La scelta può essere **deterministica**: viene scelta la parola con la **probabilità massima**.\\\n",
    "Oppure **semi-randomica**: una parola viene scelta in base alla **distribusione di probabilità**, consentendo la generazione di frasi diverse.\n",
    "\n",
    "#### Temperatura\n",
    "La \"randomicità\" della scelta può essere regolata in base alla **temperatura**, che altera la distribuzione di probabilità con una funzione **softmax**.\\\n",
    "Softmax consente di **intensificare** o **smorzare** la probabilità delle parole, in base alla temperatura [1,1000].\\\n",
    "Per temp=0, la scelta viene forzata in modo deterministico; per temp bassa, la scelta è quasi deterministica; per temp alta, la scelta è randomica.\\\n",
    "Empiricamente: per temp=1000, otteniamo una **distribuzione uniforme**; tuttavia già per temp=100, si ottengono diverse frasi insensate.\n",
    "\n",
    "#### Contesto Iniziale\n",
    "Se non viene fornito un contesto iniziale, si parte con n-1 token `<S>` .\n",
    "Se viene fornito un contesto iniziale, si parte dalle sue ultime n-1 parole; se è troppo corto, vengono aggiunti token `<S>` fino a raggiungere la lunghezza n-1.\\\n",
    "La presenza di un contesto inserito dall'utente crea un problema aggiuntivo: il contesto potrebbe **non essere presente nel dataset**.\\\n",
    "In questo caso, per predire la parola successiva, si cerca di utilizzare un **contesto più breve** utilizzando un (n-1)-gramma.\\\n",
    "Ad esempio, (*some,people,do*) potrebbe non essere presente, ma (*people,do*) potrebbe esserci.\\\n",
    "Se il contesto più piccolo (una singola parola) non è presente, significa che la parola non è presente in tutto il dataset, ovvero è una **parola sconosciuta**\\\n",
    "In questo caso, si cerca di stimare la probabilità di una parola successiva data una parola sconosciuta $P(word|\\text{UNK}) = \\frac{C(\\text{UNK},word)}{C(\\text{UNK})}$\\\n",
    "Per questa stima sono state utilizzate le parole che **occorrno una sola volta** nel dataset considerandole \"sconosciute\"\n",
    "\n",
    "\n",
    "### Risultati\n",
    "Il **linguaggio \"ripetitivo\"** di Donald Trump riesce ad essere colto sufficentemente bene da uno **struemento semplice** come gli n-grammi e con un dataset così ristretto.\\\n",
    "L'utilizzo della **temperatura** per controllare la varianza nella scelta delle parole, permette di ottenere diverse frasi non solo sensate, ma anche plausibili per il personaggio.\\\n",
    "La possibilità di inserire un **contesto inziale** a piacere incrementa ulteriormente la varietà di frasi generabili.\\\n",
    "Alcuni esempi significativi sono stati raccolti in *TrumpTweets_results.txt*\\\n",
    "Empiricamente **i risultati migliori** sono stati ottenuti **con temp [1,50]**. \\\n",
    "Purtroppo per n sempre maggiore non si hanno risultati più soddisfacenti, ma il sistema inzia a ripetere intere frasi del dataset.\\\n",
    "Il codice potrebbe essere computazionalmente ottimizzato per evitare il calcolo di probabilità già precedentemente calcolate o per evitare di cercare contesti smepre più piccoli, ma che già si sa presentano parole sconosciute all'interno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "\n",
    "tweets_df=pd.read_csv(\"../data/tweets_clean.csv\",quoting=3)\n",
    "tweets=tweets_df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'ciao', '”', '-', 'disse', '@pippo', 'come', 'va', '?', '.....', 'rispose', \"l'altro\", ';', 'vai', 'su', ':', 'https://www.example.com?', '#Buonaidea', '!']\n"
     ]
    }
   ],
   "source": [
    "#funzione per tokenizzare una frase in modo opportuno\n",
    "def tokenize(sentence):\n",
    "    regex_patterns = [\n",
    "        r'(?:https?://\\S+)',  # Link\n",
    "        r'(?:@\\w+|#\\w+)',      # Hashtag o Mention\n",
    "        r'(?:\\.{2,})',        # Insiemi di punti\n",
    "        r\"(?:[A-Za-z0-9&]+(?:[-’']+[A-Za-z0-9&]+)*)\",  # Parole contratte\n",
    "        r'(?:[.!?;:“”\\-])'  # Punteggiature\n",
    "    ]\n",
    "    combined_pattern = '|'.join(regex_patterns)\n",
    "    tokens = re.findall(combined_pattern, sentence)\n",
    "    return tokens\n",
    "\n",
    "#Esempio di utilizzo\n",
    "test_sentence = \"“ciao” -disse @pippo \\\"come va?.....\\\" rispose l'altro; vai su : https://www.example.com? #Buonaidea! \"\n",
    "print(tokenize(test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('<S>',), 'ciao'), (('ciao',), 'a'), (('a',), 'tutti'), (('tutti',), '</S>')]\n"
     ]
    }
   ],
   "source": [
    "#data una frase ritorna una lista di tuple di n grammi nella forma [(context, word), ...]  in cui context è formato da n-1 token e word è il token successivo\n",
    "#vengono aggiunti n-1 token <S> e </S> all'inizio e alla fine della frase\n",
    "#esempio per n=3 \"ciao a tutti\"  [(('<S>', '<S>'), 'ciao'), (('<S>', 'ciao'), 'a'), (('ciao', 'a'), 'tutti'), (('a', 'tutti'), '</S>'),(('tutti', '</S>'), '</S>')]\n",
    "def get_ngrams(sentence,n): \n",
    "    if n<2: n=2 #forzo n>=2  (non ha senso avere meno di una parola come contesto)\n",
    "    tokens = (n-1)*['<S>']+tokenize(sentence)+(n-1)*['</S>']\n",
    "    ngrams = []\n",
    "    for i in range(n - 1, len(tokens)):\n",
    "        context = tuple(tokens[i-(n-1)+j] for j in range(n - 1))   #dato l'indice i, torno indietro di n-1 token e prendo i token successivi\n",
    "        ngrams.append((context, tokens[i]))\n",
    "    return ngrams\n",
    "\n",
    "#esempio di utilizzo\n",
    "print(get_ngrams(\"ciao a tutti\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcola le occorenze per ogni ngramma ((context),word) e per ogni contesto (context)\n",
    "#se n=1 considera solo le parole con occorenza 1 (sconosciute) e le raggruppa sotto il contesto ('UNK',). ignora le parole con occorenza >1\n",
    "def counts_occurence(tweets,n=3):\n",
    "    ngrams_counts={} #occorenze degli ngrammi ((context),word) \n",
    "    context_counts={}#occorenze dei contesti (context)\n",
    "\n",
    "    if n==1: #se n=1 ottengo la lista di parole sconosciute\n",
    "        words= ' '.join(tweets).lower().split()  \n",
    "        words_count = Counter(words)  \n",
    "        unk_words = [word for word, count in words_count.items() if count == 1]\n",
    "\n",
    "    #per ogni tweet ottengo gli n-grammi e aggiorno i contatori\n",
    "    for tweet in tweets:\n",
    "        ngrams=get_ngrams(tweet,n)\n",
    "        for context, word in ngrams:\n",
    "            if n == 1:#se n=1 considero le parole sconosciute\n",
    "                if context[0] in unk_words:\n",
    "                    context = ('UNK',)\n",
    "                else:\n",
    "                    continue  #salto le parole conosciute\n",
    "            #incremento i contatori    \n",
    "            ngrams_counts[(context, word)] = ngrams_counts.get((context, word), 0) + 1\n",
    "            context_counts[context] = context_counts.get(context, 0) + 1\n",
    "    return context_counts, ngrams_counts\n",
    "\n",
    "#Esempio di utilizzo\n",
    "context_counts, ngrams_counts = counts_occurence(tweets,2)\n",
    "print(ngrams_counts)\n",
    "print(context_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probabilità 2-gram=$p(w_i|w_{i-1})= \\frac{C(w_{i-1},w_i)}{C(w_{i-1})}$\n",
    "\n",
    "##### Probabilità n-gram $p(w_i|w_{i-1}...w_{i-(n-1)})= \\frac{C(w_{i-(n-1)}...w_{i-1},w_i)}{C(w_{i-1}...w_{i-(n-1)})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#date le occorenze di un contesto e di un ngramma calcola la probabilità P(word|context) = count(context,word) / count(context)\n",
    "#se le occorenze riguardavano le parole sconosciure calcola la probabilita P(word|UNK) = count(UNK,word) / count(UNK)\n",
    "#ritorna un dizionario di probabilità {ngramma: probabilità}\n",
    "def calculate_probabilities(context_counts, ngrams_counts):\n",
    "    ngrams_prob= {}# P(ngramma) = count(ngramma) / count(context)\n",
    "    for ngram, ngram_count in ngrams_counts.items():\n",
    "        context = ngram[0]\n",
    "        ngrams_prob[ngram] = ngram_count / context_counts[context]\n",
    "    return ngrams_prob\n",
    "        \n",
    "ngrams_prob = calculate_probabilities(context_counts, ngrams_counts)\n",
    "print(ngrams_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 2 I have no idea how a loser !\n",
      "n = 3 The Fake News Losers ! https://t.co/3RHcBZogms\n",
      "n = 4 I have many great people but also an amazing number of haters and losers !\n",
      "n = 5 @Ashley33331 True but the losers don't know that - thanks .\n",
      "n = 6 I have many great people but also an amazing number of haters and losers responding to my tweets-why do these lowlifes follow-nothing to do !\n",
      "n = 7 I have watched sloppy Graydon Carter fail and close Spy Magazine and now am watching him fail at @VanityFair Magazine . He is a total loser !\n",
      "n = 8 I have nothing to do with Atlantic City-sold years ago great timing . For losers and haters I NEVER went bankrupt . Plus 10 billion sorry\n",
      "n = 9 I have nothing to do with Atlantic City-sold years ago great timing . For losers and haters I NEVER went bankrupt . Plus 10 billion sorry\n"
     ]
    }
   ],
   "source": [
    "#data una lista di probabilità, resituisce una lista alterata in base alla temperatura [1-1000]\n",
    "#se la temperatura è alta, le probabilità vengono appiattite (distribuzione uniforme), se è bassa le probabilità vengono esaltate\n",
    "def softmax(x, temperature):\n",
    "    temperature=temperature/1000\n",
    "    e_x = np.exp((x - np.max(x)) / temperature)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#data una porzione di testo e un dizionario di probabilità, genera la parola successiva usando un n gramma\n",
    "#la scelta della parola può essere deterministica (temp=0) o semi-randomica in base alla temperatura, (più è alta più la scelta è randomica)\n",
    "#se il contesto per ngramma non è presente nel dizionario di probabilità, viene provato a generare una parola con un contesto più corto (n-1)\n",
    "#quando n=1 si considera il contesto ('UNK',) e viene generata una parola in base alla distribuzione delle parole che seguono le parole sconosciute(con occorenza 1)\n",
    "def generate_word(n,text, ngrams_prob=None, temp=0):\n",
    "    #se non è presente il dizionario di probabilità viene calcolato\n",
    "    if ngrams_prob is None:\n",
    "        context_counts, ngrams_counts = counts_occurence(tweets,n)\n",
    "        ngrams_prob = calculate_probabilities(context_counts, ngrams_counts)\n",
    "    \n",
    "    if n==1: context = ('UNK',) #se n=1, la parola è sconosciuta\n",
    "    else: context = tuple(text[-(n-1):]) #prendo gli ultimi n-1 token del testo\n",
    "\n",
    "    #se il contesto non è presente nel testo provo a generare una parola con un contesto più corto\n",
    "    if [ngram for ngram in ngrams_prob if ngram[0] == context]==[] :\n",
    "            return generate_word(n-1,text, ngrams_prob=None, temp=temp) \n",
    "    else:#genero la parola\n",
    "        if temp==0:#scelta deterministica, prendo l'ngramma con probabilità massima dato il contesto\n",
    "            best_ngram = max((ngram for ngram in ngrams_prob if ngram[0] == context), key=lambda ngram: ngrams_prob[ngram])\n",
    "        else:#scelta semi-randomica, scelgo l'ngramma usando le probabilità come pesi alterati dalla temperatura\n",
    "            ngrams=[ngram for ngram in ngrams_prob if ngram[0] == context]\n",
    "            probs=[ngrams_prob[ngram] for ngram in ngrams]\n",
    "            probs=softmax(probs,temp)\n",
    "            best_ngram = random.choices(ngrams, weights=probs, k=1)[0]\n",
    "    return best_ngram[1]\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#data una porzione di testo, genera un testo di lunghezza massima max_len o fino a che non viene generato </S>\n",
    "def generate_text(context=\"\",n=3, max_len=10,temp=0):\n",
    "    context_counts, ngrams_counts = counts_occurence(tweets,n)\n",
    "    ngrams_prob = calculate_probabilities(context_counts, ngrams_counts)\n",
    "\n",
    "    #inzializzazione del contesto iniziale (aggiunta di token <S>)\n",
    "    context=tokenize(context)\n",
    "    if len(context)<n-1:\n",
    "        text = (n-1-len(context))*['<S>']+context\n",
    "    else:\n",
    "        text = context\n",
    "\n",
    "    #generazione del testo\n",
    "    for i in range(max_len):\n",
    "        next_word = generate_word(n,text, ngrams_prob, temp)\n",
    "        text.append(next_word)\n",
    "        if next_word == '</S>': #se viene generato il token finale interrompo\n",
    "            break     \n",
    "\n",
    "    text_filtered = [item for item in text if (item != '</S>' and item != '<S>')]\n",
    "    return ' '.join(text_filtered)\n",
    "\n",
    "#esempio di utilizzo\n",
    "for i in range(2,10):\n",
    "    print(\"n = \"+str(i),generate_text(context=\"\",n=i,max_len=30,temp=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TLNenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
