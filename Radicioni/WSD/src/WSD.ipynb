{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import semcor\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk import Tree\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[[c for c in s] for s in semcor.tagged_sents(tag='both')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Prepocessing\n",
    "Il codice seguente contiene una serie di funzioni per estrarre e preprocessare le frasi contenute in semcor\n",
    "in questo modo sarà più semplice accedere alle frasi annotate con pos tag e synset ed estrarre una parola a caso da disanabiguare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converte i pos tag di nltk in quelli di wordnet\n",
    "def convert_pos(pos):\n",
    "    pos_map = {'J': 'a', 'N': 'n', 'R': 'r', 'V': 'v', 'M': 'v'}\n",
    "    return pos_map.get(pos[0], pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertitore da semcor in dizionario\n",
    "#data una frase annotata del corpus semcor nella forma di lista di alberi\n",
    "#ritorna una lista di dizionari con la parola, il pos tag e il synset\n",
    "#vengono ignorati tutte le stopwords e punteggiature\n",
    "#gli alberi inziali per le parole con lemma sono nella nella forma  ROOT[label(lemma)]-->child[label(posTag)]-->child[word]\n",
    "#gli alberi iniziali per le parole senza lemma sono nella forma ROOT[label(posTag)]-->child[word]\n",
    "def extract_info(list):\n",
    "    token_list = []\n",
    "    for tree in list:\n",
    "        token={\"word\":None,\"pos\":None,\"syn\":None}\n",
    "        label=tree.label()\n",
    "        if isinstance(label,Lemma): #se il token ha un lemma ne ricavo il synset\n",
    "            token[\"syn\"]=label.synset()\n",
    "            tree=tree[0]#esploro il figlio che contiene la parola e il pos tag\n",
    "\n",
    "        #ignoro la punteggiatura e le parole composte/entità (es. New York)\n",
    "        if not isinstance(tree[0],Tree) and label is not None:\n",
    "            token[\"pos\"]=convert_pos(tree.label())\n",
    "            token[\"word\"]=tree[0].lower()\n",
    "            token_list.append(token)\n",
    "\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ritorna una parola casuale e la frase da cui è stata estratta, se only_nouns=True ritorna necessariamente un sostantivo\n",
    "def get_random_word(data,only_nouns=False):\n",
    "    found=False\n",
    "    while not found:\n",
    "        sentence=random.choice(data)\n",
    "        token_list=extract_info(sentence)\n",
    "        content_word=[token for token in token_list if token[\"syn\"]is not None]#non estraggo le stopwords\n",
    "        for token in content_word:\n",
    "            if wn.synsets(token[\"word\"]) != []:#se la parola ha almeno un synset\n",
    "                if not only_nouns or token[\"pos\"]==\"n\":\n",
    "                        return (token,token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Implementazione WSD\n",
    "IL codice seguente costituisce la parte principale del progetto, contiene le funzioni per ottenere il contesto, la signature, per implementare l'overlap e l'algoritmo di Lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepocessa i dati, rimuovendo la punteggiatura, tokenizzando e lemmatizzando\n",
    "def preprocess(sentence): \n",
    "    if isinstance(sentence,str):#se è una stringa la tokenizzo in lista\n",
    "        sentence=word_tokenize(sentence)\n",
    "    sentence=[w.lower() for w in sentence if w not in string.punctuation]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens=[lemmatizer.lemmatize(w) for w in sentence]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data una lista di synset e le frasi annotate di semcor\n",
    "#controlla ogni frase che contiene un certo synset\n",
    "#ritorna un dizionario, in cui ad ogni synset è associata una lita di frasi tokenizzate in cui compare\n",
    "#corpus_examples= {syn1: [[esempio1],[esempio2],[esempio3]], syn2:[...]}\n",
    "def get_corpus_examples(synsets,data):\n",
    "    corpus_examples={syn.name():[] for syn in synsets}\n",
    "    for sentence in data:\n",
    "        token_list=extract_info(sentence)\n",
    "        for token in token_list:\n",
    "            syn=token[\"syn\"]\n",
    "            if syn is not None and syn in synsets:\n",
    "                corpus_examples[syn.name()].append([t[\"word\"]for t in token_list])\n",
    "                break\n",
    "    return corpus_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per ogni parola in word_list incrementa il numero di documenti che contengono la parola\n",
    "#se la parola non è presente viene aggiunta \n",
    "def add_to_signature(signature_dict,word_list):\n",
    "    for word in set(word_list): #non considero duplicati nella stessa frase (?)\n",
    "        signature_dict[word]=signature_dict.get(word,0)+1\n",
    "    return signature_dict\n",
    "\n",
    "#ritorna la signature di un synset\n",
    "#la signature è un dizionario che associa ad ogni parola il suo peso (idf) calcolato come  idf_i=log(Ndoc/Nd_i)\n",
    "def get_signature(synset,corpus_examples=None,gemini_examples=None):\n",
    "    signature={}\n",
    "\n",
    "    #----------------vengono aggiunta la definizione e  li esempi di wordnet---------------\n",
    "    Ndoc=1# definizione del synset\n",
    "    signature=add_to_signature(signature,preprocess(synset.definition()))#descrizione del synset\n",
    "    for wn_ex in synset.examples(): #esempi su wordnet\n",
    "        signature=add_to_signature(signature,preprocess(wn_ex))\n",
    "        Ndoc+=1\n",
    "\n",
    "    #----------------vengono aggiunti gli esempi del corpus-----------------------\n",
    "    if corpus_examples:#se sono presenti dei degli esempi del corpus\n",
    "        for corpus_ex in corpus_examples:#frasi su semcor in cui compare il synset\n",
    "            signature=add_to_signature(signature,preprocess(corpus_ex))\n",
    "            Ndoc+=1\n",
    "\n",
    "    #----------------vengono aggiunti gli esempi di gemini-----------------------\n",
    "    if gemini_examples:#se sono presenti dei degli esempi di gemini\n",
    "        for gemini_ex in gemini_examples:\n",
    "            signature=add_to_signature(signature,preprocess(gemini_ex))\n",
    "            Ndoc+=1\n",
    "            \n",
    "    #calcolo dell'idf\n",
    "    for word in signature:\n",
    "        signature[word]=np.log(Ndoc/signature[word])\n",
    "    sorted_signature = dict(sorted(signature.items(), key=lambda item: item[1],reverse=True))\n",
    "    return sorted_signature\n",
    "\n",
    "#ritorna il contesto di una parola\n",
    "#il contesto è composto dalla lista di parole lemmizzate della frase in cui si trova la parola da disambiguare\n",
    "#se la parola è una content word, utilizzo il suo pos tag per aiutare il lematizer\n",
    "#altrimenti è una stopword e non viene lemmaizzata\n",
    "def get_context(token_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(t[\"word\"], t[\"pos\"]) if t[\"syn\"] is not None else t[\"word\"] for t in token_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcola l'overlap tra il contesto e la signature di un synset\n",
    "#se weighted=True calcola la somma dei pesi delle parole in comune \n",
    "#se weighted=False calcola il numero di parole in comune eccetto le stopwords\n",
    "def compute_overlap(signature,context,weighted=False):\n",
    "    if weighted:\n",
    "        return sum([signature.get(word,0) for word in context])\n",
    "    else:\n",
    "        stopword = stopwords.words('english')\n",
    "        #remove stopwords\n",
    "        context= [w for w in context if w.casefold() not in stopword]\n",
    "        signature= {k:v for k,v in signature.items() if k not in stopword}\n",
    "        return len(set(signature.keys()).intersection(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementazione dell'algoritmo di Lesk\n",
    "#ritorna il synset che ha il massimo overlap tra la signature e il contesto\n",
    "def lesk(word, pos, sentence, data=None, gemini=False):\n",
    "    synsets = wn.synsets(word, pos)#\n",
    "    if synsets == []: synsets=wn.synsets(word)#se non trovo il synset con il pos tag specificato uso tutti i synset\n",
    "    max_overlap = 0\n",
    "    best_syns = synsets[0]\n",
    "    context = get_context(sentence)\n",
    "\n",
    "    #-----------------se specificato vengono estratti altri esempi (corupus e/o gemini)---------------------\n",
    "    if data: \n",
    "        corpus_examples = get_corpus_examples(synsets, data)\n",
    "    else: corpus_examples = None\n",
    "    if gemini:\n",
    "        gemini_examples=get_gemini_examples(synsets) #parte opzionale in fondo\n",
    "    else: gemini_examples=None\n",
    "\n",
    "    #----------------ricerca del synset con overlap maggiore---------------------\n",
    "    for syn in synsets:\n",
    "        signature = get_signature(syn, \n",
    "                                  corpus_examples.get(syn.name(),None) if corpus_examples else None,\n",
    "                                  gemini_examples.get(syn.name(),None) if gemini_examples else None)\n",
    "        overlap = compute_overlap(signature, context, weighted=bool(data))\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_syns = syn\n",
    "\n",
    "    return best_syns\n",
    "\n",
    "#baseline, ritorna il synset più frequente\n",
    "def naive_wsd(word):\n",
    "    synsets=wn.synsets(word)\n",
    "    if synsets==[]: return None\n",
    "    else:return synsets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esempio di utilizzo delle funzioni precedenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# word:  notion\n",
      "# sentence:  ['the', 'notion', 'of', 'inspiration', 'is', 'somehow', 'cognate', 'to', 'this', 'feeling']\n",
      "# extracted context:  ['the', 'notion', 'of', 'inspiration', 'be', 'somehow', 'cognate', 'to', 'this', 'feeling']\n",
      "# signature of each synset (No corpus examples):\n",
      " - impression.n.01  overlap:  1  signature:  {'vague': 1.6094379124341003, 'some': 1.6094379124341003, 'is': 1.6094379124341003, 'placed': 1.6094379124341003, 'which': 1.6094379124341003, 'confidence': 1.6094379124341003, 'idea': 1.6094379124341003, 'of': 1.6094379124341003, 'her': 1.6094379124341003, 'favorable': 1.6094379124341003, 'impression': 1.6094379124341003, 'about': 1.6094379124341003, 'crisis': 1.6094379124341003, 'the': 1.6094379124341003, 'what': 1.6094379124341003, 'are': 1.6094379124341003, 'your': 1.6094379124341003, 'my': 1.6094379124341003, 'it': 1.6094379124341003, 'belief': 1.6094379124341003, 'sincerity': 1.6094379124341003, 'strengthened': 1.6094379124341003, 'she': 1.6094379124341003, 'had': 1.6094379124341003, 'that': 1.6094379124341003, 'lying': 1.6094379124341003, 'i': 1.6094379124341003, 'in': 0.9162907318741551, 'a': 0.9162907318741551, 'his': 0.9162907318741551, 'wa': 0.9162907318741551, 'feeling': 0.9162907318741551}\n",
      " - notion.n.02  overlap:  0  signature:  {'inclusive': 0.0, 'concept': 0.0, 'general': 0.0, 'a': 0.0}\n",
      " - notion.n.03  overlap:  1  signature:  {'capricious': 1.3862943611198906, 'fanciful': 1.3862943611198906, 'an': 1.3862943611198906, 'odd': 1.3862943611198906, 'or': 1.3862943611198906, 'idea': 1.3862943611198906, 'in': 1.3862943611198906, 'story': 1.3862943611198906, 'of': 1.3862943611198906, 'associated': 1.3862943611198906, 'his': 1.3862943611198906, 'is': 1.3862943611198906, 'disaster': 1.3862943611198906, 'notion': 1.3862943611198906, 'disguise': 1.3862943611198906, 'theatrical': 1.3862943611198906, 'about': 1.3862943611198906, 'flying': 1.3862943611198906, 'had': 1.3862943611198906, 'moon': 1.3862943611198906, 'he': 1.3862943611198906, 'a': 1.3862943611198906, 'enjoy': 1.3862943611198906, 'it': 1.3862943611198906, 'can': 1.3862943611198906, 'be': 1.3862943611198906, 'humorous': 1.3862943611198906, 'someone': 1.3862943611198906, 'time': 1.3862943611198906, 'with': 0.6931471805599453, 'the': 0.6931471805599453, 'to': 0.6931471805599453, 'whimsy': 0.6931471805599453}\n",
      " - notion.n.04  overlap:  1  signature:  {'sewing': 0.6931471805599453, 'small': 0.6931471805599453, 'article': 0.6931471805599453, 'clothing': 0.6931471805599453, 'usually': 0.6931471805599453, 'or': 0.6931471805599453, 'item': 0.6931471805599453, 'plural': 0.6931471805599453, 'personal': 0.6931471805599453, 'button': 0.6931471805599453, 'needle': 0.6931471805599453, 'notion': 0.6931471805599453, 'are': 0.6931471805599453, 'and': 0.6931471805599453}\n",
      "\n",
      "# signature of each synset (with corpus examples):\n",
      " - impression.n.01  overlap: 7.5941  signature:  {'placed': 4.110873864173311, 'confidence': 4.110873864173311, 'her': 4.110873864173311, 'favorable': 4.110873864173311, 'crisis': 4.110873864173311, 'sincerity': 4.110873864173311, 'strengthened': 4.110873864173311, 'she': 4.110873864173311, 'sitting': 4.110873864173311, 'instrument': 4.110873864173311, 'wall': 4.110873864173311, 'however': 4.110873864173311, 'right': 4.110873864173311, 'facing': 4.110873864173311, 'room': 4.110873864173311, 'lined': 4.110873864173311, 'globe': 4.110873864173311, 'country': 4.110873864173311, 'movie': 4.110873864173311, 'meet': 4.110873864173311, 'corner': 4.110873864173311, 'mysterious': 4.110873864173311, 'cast': 4.110873864173311, 'upon': 4.110873864173311, 'presence': 4.110873864173311, 'spell': 4.110873864173311, 'evil': 4.110873864173311, 'leave': 4.110873864173311, 'added': 4.110873864173311, 'accounting': 4.110873864173311, 'analyze': 4.110873864173311, 'together': 4.110873864173311, 'wickedness': 4.110873864173311, 'produce': 4.110873864173311, 'cumulative': 4.110873864173311, 'enough': 4.110873864173311, 'intention': 4.110873864173311, 'absurd': 4.110873864173311, 'mythological': 4.110873864173311, 'demythologization': 4.110873864173311, 'completely': 4.110873864173311, 'misrepresents': 4.110873864173311, 'entail': 4.110873864173311, 'concept': 4.110873864173311, 'expurgation': 4.110873864173311, 'somewhat': 4.110873864173311, 'startling': 4.110873864173311, 'jewish': 4.110873864173311, 'lead': 4.110873864173311, 'friday': 4.110873864173311, 'sabbath': 4.110873864173311, 'adam': 4.110873864173311, 'day': 4.110873864173311, 'see': 4.110873864173311, 'disobeyed': 4.110873864173311, 'eve': 4.110873864173311, 'namely': 4.110873864173311, 'good': 4.110873864173311, 'christ': 4.110873864173311, 'death': 4.110873864173311, 'died': 4.110873864173311, 'preparation': 4.110873864173311, 'parallel': 4.110873864173311, 'explain': 4.110873864173311, 'strangest': 4.110873864173311, 'very': 4.110873864173311, 'already': 4.110873864173311, 'psychiatrist': 4.110873864173311, 'seen': 4.110873864173311, 'coincidence': 4.110873864173311, 'dismiss': 4.110873864173311, 'illusion': 4.110873864173311, 'just': 4.110873864173311, 'suggest': 4.110873864173311, 'realistic': 4.110873864173311, 'original': 4.110873864173311, 'fit': 4.110873864173311, 'unbroken': 4.110873864173311, 'nothing': 4.110873864173311, 'alter': 4.110873864173311, 'including': 4.110873864173311, 'inevitably': 4.110873864173311, 'explaining': 4.110873864173311, 'display': 4.110873864173311, 'call': 4.110873864173311, 'fantastic': 4.110873864173311, 'photographic': 4.110873864173311, 'component': 4.110873864173311, 'detail': 4.110873864173311, 'mystery': 4.110873864173311, 'composed': 4.110873864173311, 'job': 4.110873864173311, 'professor': 4.110873864173311, '2': 4.110873864173311, 'perfect': 4.110873864173311, 'determines': 4.110873864173311, 'vaguely': 4.110873864173311, 'ratio': 4.110873864173311, 'forgetfulness': 4.110873864173311, 'fulfilled': 4.110873864173311, 'whether': 4.110873864173311, 'degree': 4.110873864173311, 'recognized': 4.110873864173311, 'corp': 4.110873864173311, 'frequent': 4.110873864173311, 'verbal': 4.110873864173311, 'recognition': 4.110873864173311, 'barrage': 4.110873864173311, 'entirely': 4.110873864173311, 'impressive': 4.110873864173311, 'withstands': 4.110873864173311, 'novel': 4.110873864173311, 'nevertheless': 4.110873864173311, 'notably': 4.110873864173311, 'if': 4.110873864173311, 'told': 4.110873864173311, 'academic': 4.110873864173311, 'convention': 4.110873864173311, 'informed': 4.110873864173311, 'identical': 4.110873864173311, 'session': 4.110873864173311, 'victim': 4.110873864173311, 'studied': 4.110873864173311, 'leavening': 4.110873864173311, 'certainly': 4.110873864173311, 'graduate': 4.110873864173311, 'among': 4.110873864173311, 'who': 4.110873864173311, 'north': 4.110873864173311, 'throughout': 4.110873864173311, 'liberalism': 4.110873864173311, 'south': 4.110873864173311, 'strong': 4.110873864173311, 'rigid': 4.110873864173311, 'mistake': 4.110873864173311, 'decisive': 4.110873864173311, 'persists': 4.110873864173311, 'even': 4.110873864173311, 'execution': 4.110873864173311, 'fear': 4.110873864173311, 'because': 4.110873864173311, 'really': 4.110873864173311, 'never': 4.110873864173311, 'unaware': 4.110873864173311, 'quite': 4.110873864173311, 'staved': 4.110873864173311, 'daring': 4.110873864173311, 'element': 4.110873864173311, 'without': 4.110873864173311, 'adding': 4.110873864173311, 'war': 4.110873864173311, 'do': 4.110873864173311, 'insane': 4.110873864173311, 'himself': 4.110873864173311, 'person': 4.110873864173311, 'confronted': 4.110873864173311, 'drunk': 4.110873864173311, 'finally': 4.110873864173311, 'costume': 4.110873864173311, 'catastrophe': 4.110873864173311, 'singularity': 4.110873864173311, 'dressing': 4.110873864173311, 'characteristically': 4.110873864173311, 'disguise': 4.110873864173311, 'theatrical': 4.110873864173311, 'story': 4.110873864173311, 'perversely': 4.110873864173311, 'erotic': 4.110873864173311, 'change': 4.110873864173311, 'associated': 4.110873864173311, 'muscular': 4.110873864173311, 'clinical': 4.110873864173311, 'either': 4.110873864173311, 'polymyositis': 4.110873864173311, 'kind': 4.110873864173311, 'believe': 4.110873864173311, 'another': 4.110873864173311, 'fitting': 4.110873864173311, 'u': 4.110873864173311, 'certain': 4.110873864173311, 'argue': 4.110873864173311, 'yes': 4.110873864173311, 'well': 4.110873864173311, 'silent': 4.110873864173311, 'literature': 4.110873864173311, 'spite': 4.110873864173311, 'perhaps': 4.110873864173311, 'against': 4.110873864173311, 'could': 4.110873864173311, 'yourself': 4.110873864173311, 'storing': 4.110873864173311, 'walking': 4.110873864173311, 'looked': 4.110873864173311, 'street': 4.110873864173311, 'started': 4.110873864173311, 'peering': 4.110873864173311, 'how': 4.110873864173311, 'fresh': 4.110873864173311, 'passing': 4.110873864173311, 'stall': 4.110873864173311, 'shopping': 4.110873864173311, 'moved': 4.110873864173311, 'up': 4.110873864173311, 'mania': 4.110873864173311, 'making': 4.110873864173311, 'contemplation': 4.110873864173311, 'mind': 4.110873864173311, 'print': 4.110873864173311, 'leaf': 4.110873864173311, 'surpassed': 4.110873864173311, 'delight': 4.110873864173311, 'served': 4.110873864173311, 'flower': 4.110873864173311, 'scholarly': 4.110873864173311, 'spatter': 4.110873864173311, 'during': 4.110873864173311, 'renewed': 4.110873864173311, 'summer': 4.110873864173311, 'plaster': 4.110873864173311, 'glad': 4.110873864173311, 'such': 4.110873864173311, 'scriptural': 4.110873864173311, 'smiled': 4.110873864173311, 'am': 4.110873864173311, 'lesson': 4.110873864173311, 'besides': 4.110873864173311, 'meant': 4.110873864173311, 'worse': 4.110873864173311, 'get': 4.110873864173311, 'thanks': 4.110873864173311, 'better': 4.110873864173311, 'got': 4.110873864173311, 'thing': 4.110873864173311, 'why': 4.110873864173311, 'reached': 4.110873864173311, 'veteran': 4.110873864173311, 'frontier': 4.110873864173311, 'last': 4.110873864173311, 'so-called': 4.110873864173311, 'two': 4.110873864173311, 'year': 4.110873864173311, 'theatregoer': 4.110873864173311, 'psyche': 4.110873864173311, 'bittersweet': 4.110873864173311, 'produced': 4.110873864173311, 'indelible': 4.110873864173311, 'drama': 4.110873864173311, 'art': 4.110873864173311, 'number': 4.110873864173311, 'widespread': 4.110873864173311, 'god': 4.110873864173311, 'share': 4.110873864173311, 'probably': 4.110873864173311, 'now': 4.110873864173311, 'wide': 4.110873864173311, 'painting': 4.110873864173311, 'eye': 4.110873864173311, 'closed': 4.110873864173311, 'half': 4.110873864173311, \"mind's\": 4.110873864173311, 'physical': 4.110873864173311, 'account': 4.110873864173311, 'convey': 4.110873864173311, 'wish': 4.110873864173311, 'anti-semitic': 4.110873864173311, 'stubbornly': 4.110873864173311, 'definition': 4.110873864173311, 'anti-semitism': 4.110873864173311, 'private': 4.110873864173311, 'insisting': 4.110873864173311, 'denying': 4.110873864173311, 'contended': 4.110873864173311, 'arena': 4.110873864173311, 'unavoidable': 4.110873864173311, 'large': 4.110873864173311, 'say': 4.110873864173311, 'longer': 4.110873864173311, 'movement': 4.110873864173311, 'own': 4.110873864173311, 'interrelated': 4.110873864173311, 'vast': 4.110873864173311, 'move': 4.110873864173311, 'influence': 4.110873864173311, 'simply': 4.110873864173311, 'oneself': 4.110873864173311, 'view': 4.110873864173311, 'unity': 4.110873864173311, 'arranged': 4.110873864173311, 'larger': 4.110873864173311, 'except': 4.110873864173311, 'take': 4.110873864173311, 'collection': 4.110873864173311, 'abstraction': 4.110873864173311, 'themselves': 4.110873864173311, 'pattern': 4.110873864173311, 'sequence': 4.110873864173311, 'infuriating': 4.110873864173311, 'continually': 4.110873864173311, 'picked': 4.110873864173311, 'popular': 4.110873864173311, 'then': 4.110873864173311, 'tremendous': 4.110873864173311, 'revulsion': 4.110873864173311, 'suddenly': 4.110873864173311, 'between': 4.110873864173311, 'polarity': 4.110873864173311, 'balanced': 4.110873864173311, 'describes': 4.110873864173311, 'called': 4.110873864173311, 'became': 4.110873864173311, 'social': 4.110873864173311, 'philosophy': 4.110873864173311, 'after': 4.110873864173311, 'french': 4.110873864173311, 'may': 4.110873864173311, 'general': 4.110873864173311, 'twist': 4.110873864173311, 'rousseau': 4.110873864173311, 'sort': 4.110873864173311, 'counter': 4.110873864173311, 'stereotyped': 4.110873864173311, 'demagogue': 4.110873864173311, 'communist': 4.110873864173311, 'lurking': 4.110873864173311, 'bogey': 4.110873864173311, 'found': 4.110873864173311, 'behind': 4.110873864173311, 'safe': 4.110873864173311, 'perfectly': 4.110873864173311, 'defensible': 4.110873864173311, 'procedure': 4.110873864173311, 'give': 4.110873864173311, 'hence': 4.110873864173311, 'evidence': 4.110873864173311, 'repress': 4.110873864173311, 'unless': 4.110873864173311, 'distinct': 4.110873864173311, 'single': 4.110873864173311, 'valuable': 4.110873864173311, 'clarity': 4.110873864173311, 'fuzzy': 4.110873864173311, 'jew': 4.110873864173311, 'studying': 4.110873864173311, 'high': 4.110873864173311, 'young': 4.110873864173311, 'consigned': 4.110873864173311, 'scrap': 4.110873864173311, 'long': 4.110873864173311, 'conditioned': 4.110873864173311, 'child': 4.110873864173311, 'adult': 4.110873864173311, 'made': 4.110873864173311, 'mistaken': 4.110873864173311, 'feared': 4.110873864173311, 'role': 4.110873864173311, 'stepmother': 4.110873864173311, 'parent': 4.110873864173311, 'difficult': 4.110873864173311, 'antagonistic': 4.110873864173311, 'close': 4.110873864173311, 'want': 4.110873864173311, 'unborn': 4.110873864173311, 'function': 4.110873864173311, 'universally': 4.110873864173311, 'almost': 4.110873864173311, 'immediate': 4.110873864173311, 'existence': 4.110873864173311, 'someone': 4.110873864173311, 'too': 4.110873864173311, 'leap': 4.110873864173311, 'metaphor': 4.110873864173311, 'here': 4.110873864173311, 'sensitive': 4.110873864173311, 'inferential': 4.110873864173311, 'personal': 4.110873864173311, 'affair': 4.110873864173311, 'apparently': 4.110873864173311, 'practical': 4.110873864173311, 'significance': 4.110873864173311, 'pedantic': 4.110873864173311, 'technical': 4.110873864173311, 'visible': 4.110873864173311, 'interest': 4.110873864173311, 'anything': 4.110873864173311, 'involvement': 4.110873864173311, 'abstruseness': 4.110873864173311, 'vocabulary': 4.110873864173311, 'acquired': 4.110873864173311, 'tact': 4.110873864173311, 'expectable': 4.110873864173311, 'item': 4.110873864173311, 'lexical': 4.110873864173311, 'evident': 4.110873864173311, 'inconsistent': 4.110873864173311, 'adjustment': 4.110873864173311, 'precisely': 4.110873864173311, 'inhibited': 4.110873864173311, 'required': 4.110873864173311, 'market': 4.110873864173311, 'status': 4.110873864173311, 'abrogated': 4.110873864173311, 'business': 4.110873864173311, 'explicitly': 4.110873864173311, 'economics': 4.110873864173311, 'done': 4.110873864173311, 'administrative': 4.110873864173311, 'authority': 4.110873864173311, 'politics': 4.110873864173311, 'rising': 4.110873864173311, 'judge': 4.110873864173311, 'automatic': 4.110873864173311, 'supervision': 4.110873864173311, 'old': 4.110873864173311, 'trade': 4.110873864173311, 'limited': 4.110873864173311, 'obligation': 4.110873864173311, 'factor': 4.110873864173311, 'production': 4.110873864173311, 'clog': 4.110873864173311, 'custom': 4.110873864173311, 'bourgeoisie': 4.110873864173311, 'objective': 4.110873864173311, 'morality': 4.110873864173311, 'somewhere': 4.110873864173311, 'should': 4.110873864173311, 'car': 4.110873864173311, 'hide': 4.110873864173311, 'thought': 4.110873864173311, 'reprehensible': 4.110873864173311, 'hardly': 4.110873864173311, 'anticipated': 4.110873864173311, 'electric': 4.110873864173311, 'coiling': 4.110873864173311, 'muscle': 4.110873864173311, 'him': 4.110873864173311, 'nerve': 4.110873864173311, 'body': 4.110873864173311, 'out': 4.110873864173311, 'fine': 4.110873864173311, 'poured': 4.110873864173311, 'combat': 4.110873864173311, 'forever': 4.110873864173311, 'canceled': 4.110873864173311, 'dark': 4.110873864173311, 'mission': 4.110873864173311, 'muddy': 4.110873864173311, 'wet': 4.110873864173311, 'aside': 4.110873864173311, 'clothes': 4.110873864173311, 'putting': 4.110873864173311, 'changing': 4.110873864173311, 'dark-blue': 4.110873864173311, 'suit': 4.110873864173311, 'laid': 4.110873864173311, 'jacket': 4.110873864173311, 'gray': 4.110873864173311, 'flannel': 4.110873864173311, 'put': 4.110873864173311, 'my': 3.417726683613366, 'lying': 3.417726683613366, 'only': 3.417726683613366, 'often': 3.417726683613366, 'major': 3.417726683613366, 'into': 3.417726683613366, 'individual': 3.417726683613366, 'effect': 3.417726683613366, 'prevision': 3.417726683613366, 'these': 3.417726683613366, 'instance': 3.417726683613366, 'continuity': 3.417726683613366, 'experience': 3.417726683613366, 'can': 3.417726683613366, 'brain': 3.417726683613366, 'future': 3.417726683613366, 'seem': 3.417726683613366, 'must': 3.417726683613366, 'though': 3.417726683613366, 'basic': 3.417726683613366, 'theory': 3.417726683613366, 'memory': 3.417726683613366, 'circumstance': 3.417726683613366, 'effective': 3.417726683613366, 'psychologist': 3.417726683613366, 'time': 3.417726683613366, 'college': 3.417726683613366, 'especially': 3.417726683613366, 'yet': 3.417726683613366, 'ritual': 3.417726683613366, 'uneasy': 3.417726683613366, 'act': 3.417726683613366, 'every': 3.417726683613366, 'part': 3.417726683613366, 'since': 3.417726683613366, 'keep': 3.417726683613366, 'abandon': 3.417726683613366, 'behavior': 3.417726683613366, 'me': 3.417726683613366, 'when': 3.417726683613366, 'toward': 3.417726683613366, 'attitude': 3.417726683613366, 'by': 3.417726683613366, 'rest': 3.417726683613366, 'girl': 3.417726683613366, 'much': 3.417726683613366, 'while': 3.417726683613366, 'being': 3.417726683613366, 'present': 3.417726683613366, 'great': 3.417726683613366, 'sense': 3.417726683613366, 'action': 3.417726683613366, 'world': 3.417726683613366, 'given': 3.417726683613366, 'run': 3.417726683613366, 'difference': 3.417726683613366, 'language': 3.417726683613366, 'go': 3.417726683613366, 'possible': 3.417726683613366, 'away': 3.417726683613366, 'vague': 3.0122615755052013, 'some': 3.0122615755052013, 'idea': 3.0122615755052013, 'others': 3.0122615755052013, 'left': 3.0122615755052013, 'them': 3.0122615755052013, 'said': 3.0122615755052013, 'source': 3.0122615755052013, 'so': 3.0122615755052013, 'power': 3.0122615755052013, 'been': 3.0122615755052013, 'people': 3.0122615755052013, 'also': 3.0122615755052013, 'same': 3.0122615755052013, 'lived': 3.0122615755052013, 'event': 3.0122615755052013, 'past': 3.0122615755052013, 'like': 3.0122615755052013, 'we': 3.0122615755052013, 'will': 3.0122615755052013, 'our': 3.0122615755052013, 'at': 3.0122615755052013, 'those': 3.0122615755052013, 'free': 3.0122615755052013, 'make': 3.0122615755052013, 'involved': 3.0122615755052013, 'no': 3.0122615755052013, 'more': 3.0122615755052013, 'than': 3.0122615755052013, 'new': 3.0122615755052013, 'what': 2.7245795030534206, 'your': 2.7245795030534206, 'belief': 2.7245795030534206, 'you': 2.7245795030534206, 'many': 2.7245795030534206, 'having': 2.7245795030534206, 'not': 2.7245795030534206, 'through': 2.7245795030534206, 'before': 2.7245795030534206, 'dream': 2.7245795030534206, 'weird': 2.7245795030534206, 'their': 2.7245795030534206, 'ha': 2.7245795030534206, 'were': 2.7245795030534206, 'about': 2.501435951739211, 'i': 2.501435951739211, 'most': 2.501435951739211, 'deja': 2.501435951739211, 'any': 2.501435951739211, 'but': 2.501435951739211, 'might': 2.501435951739211, 'which': 2.3191143949452564, 'his': 2.3191143949452564, 'on': 2.3191143949452564, \"'s\": 2.3191143949452564, 'there': 2.3191143949452564, 'from': 2.3191143949452564, 'all': 2.164963715117998, 'would': 2.164963715117998, 'are': 2.031432322493475, 'one': 2.031432322493475, 'an': 2.031432322493475, 'for': 2.031432322493475, 'they': 1.9136492868370918, 'had': 1.8082887711792655, 'with': 1.8082887711792655, 'have': 1.8082887711792655, 'or': 1.8082887711792655, 'be': 1.8082887711792655, 'wa': 1.7129785913749407, 'this': 1.6259672143853108, 'he': 1.5459245067117746, 'impression': 1.4718165345580525, 'notion': 1.4718165345580525, 'is': 1.402823663071101, 'it': 1.402823663071101, 'to': 1.2205021062771466, 'in': 1.0663514264498881, 'and': 0.7786693539981072, 'feeling': 0.7435780341868372, 'that': 0.676886659688165, 'a': 0.6451379613735847, 'of': 0.39730179746900346, 'the': 0.32668423025505017}\n",
      " - notion.n.02  overlap: 14.8117  signature:  {'inclusive': 2.70805020110221, 'concept': 2.70805020110221, 'between': 2.70805020110221, 'freud': 2.70805020110221, 'resemblance': 2.70805020110221, 'haunting': 2.70805020110221, 'cause': 2.70805020110221, 'would': 2.70805020110221, 'point': 2.70805020110221, 'any': 2.70805020110221, 'which': 2.70805020110221, 'fitting': 2.70805020110221, 'case': 2.70805020110221, 'lost': 2.70805020110221, 'all': 2.70805020110221, 'fitness': 2.70805020110221, 'other': 2.70805020110221, 'unfitting': 2.70805020110221, 'attitude': 2.70805020110221, 'mean': 2.70805020110221, 'nor': 2.70805020110221, 'within': 2.70805020110221, 'possible': 2.70805020110221, 'u': 2.70805020110221, 'jansenist': 2.70805020110221, 'greek': 2.70805020110221, 'clarity': 2.70805020110221, 'not': 2.70805020110221, 'who': 2.70805020110221, 'later': 2.70805020110221, 'napoleon': 2.70805020110221, 'dictatorship': 2.70805020110221, 'gave': 2.70805020110221, 'brought': 2.70805020110221, 'feeling': 2.70805020110221, 'inspiration': 2.70805020110221, 'cognate': 2.70805020110221, 'somehow': 2.70805020110221, 'aristotelian': 2.70805020110221, 'viable': 2.70805020110221, 'purging': 2.70805020110221, 'catharsis': 2.70805020110221, 'persistent': 2.70805020110221, 'plato': 2.70805020110221, 'second': 2.70805020110221, 'stand': 2.70805020110221, 'discharge': 2.70805020110221, 'view': 2.70805020110221, 'opposition': 2.70805020110221, 'here': 2.70805020110221, 'arousal': 2.70805020110221, 'resolution': 2.70805020110221, 'indecision': 2.70805020110221, 'expression': 2.70805020110221, 'harmony': 2.70805020110221, 'given': 2.70805020110221, 'including': 2.70805020110221, 'advanced': 2.70805020110221, 'nature': 2.70805020110221, 'many': 2.70805020110221, 'outstanding': 2.70805020110221, 'dignity': 2.70805020110221, 'among': 2.70805020110221, 'definition': 2.70805020110221, 'been': 2.70805020110221, 'perfectibility': 2.70805020110221, 'these': 2.70805020110221, 'secondary': 2.70805020110221, 'also': 2.70805020110221, 'century': 2.70805020110221, 'human': 2.70805020110221, 'depravity': 2.70805020110221, 'itself': 2.70805020110221, 'have': 2.70805020110221, 'over': 2.70805020110221, 'manifestation': 2.70805020110221, 'art': 2.70805020110221, 'practice': 2.70805020110221, 'trace': 2.70805020110221, 'practical': 2.70805020110221, 'through': 2.70805020110221, 'theological': 2.70805020110221, 'politics': 2.70805020110221, 'may': 2.70805020110221, 'into': 2.70805020110221, 'thus': 2.70805020110221, 'reflection': 2.70805020110221, 'autonomy': 2.70805020110221, 'theory': 2.70805020110221, 'passed': 2.70805020110221, 'avoid': 2.70805020110221, 'group': 2.70805020110221, 'another': 2.70805020110221, 'mentioned': 2.70805020110221, 'even': 2.70805020110221, 'counter': 2.70805020110221, 'can': 2.70805020110221, 'those': 2.70805020110221, 'intact': 2.70805020110221, 'example': 2.70805020110221, 'suggested': 2.70805020110221, 'comparable': 2.70805020110221, 'just': 2.70805020110221, 'coin': 2.70805020110221, 'unit': 2.70805020110221, 'great': 2.70805020110221, 'ourselves': 2.70805020110221, 'united': 2.70805020110221, 'well': 2.70805020110221, 'conduct': 2.70805020110221, 'department': 2.70805020110221, 'policy': 2.70805020110221, 'established': 2.70805020110221, 'international': 2.70805020110221, 'bearing': 2.70805020110221, 'treaty': 2.70805020110221, 'paper': 2.70805020110221, 'how': 2.70805020110221, 'about': 2.70805020110221, 'charter': 2.70805020110221, 'his': 2.70805020110221, 'understood': 2.70805020110221, 'under': 2.70805020110221, 'american': 2.70805020110221, 'accumulated': 2.70805020110221, 'he': 2.70805020110221, 'take': 2.70805020110221, 'statute': 2.70805020110221, 'commitment': 2.70805020110221, 'country': 2.70805020110221, 'basic': 2.70805020110221, 'precedent': 2.70805020110221, 'where': 2.70805020110221, 'she': 2.70805020110221, 'on': 2.70805020110221, 'ordinarily': 2.70805020110221, 'center': 2.70805020110221, 'antipathy': 2.70805020110221, 'seems': 2.70805020110221, 'dominant': 2.70805020110221, 'stress': 2.70805020110221, 'philosophical': 2.70805020110221, 'recognize': 2.70805020110221, 'doctrine': 2.70805020110221, 'hegel': 2.70805020110221, 'philosopher': 2.70805020110221, 'ultimate': 2.70805020110221, 'state': 2.70805020110221, 'origin': 2.70805020110221, 'although': 2.70805020110221, 'reached': 2.70805020110221, 'especially': 2.70805020110221, 'german': 2.70805020110221, 'statement': 2.70805020110221, 'put': 2.70805020110221, 'general': 2.0149030205422647, 'there': 2.0149030205422647, 'ha': 2.0149030205422647, 'or': 2.0149030205422647, 'for': 2.0149030205422647, 'one': 2.0149030205422647, 'emotion': 2.0149030205422647, \"'s\": 2.0149030205422647, 'must': 2.0149030205422647, 'some': 2.0149030205422647, 'such': 2.0149030205422647, 'literature': 2.0149030205422647, 'it': 2.0149030205422647, 'individual': 2.0149030205422647, 'political': 2.0149030205422647, 'people': 2.0149030205422647, 'by': 2.0149030205422647, 'that': 1.6094379124341003, 'be': 1.6094379124341003, 'are': 1.6094379124341003, 'but': 1.6094379124341003, 'will': 1.6094379124341003, 'idea': 1.6094379124341003, 'from': 1.6094379124341003, 'we': 1.6094379124341003, 'a': 1.0986122886681098, 'this': 1.0986122886681098, 'to': 0.9162907318741551, 'and': 0.7621400520468967, 'is': 0.7621400520468967, 'in': 0.4054651081081644, 'of': 0.14310084364067324, 'the': 0.14310084364067324, 'notion': 0.06899287148695142}\n",
      " - notion.n.03  overlap: 6.4786  signature:  {'capricious': 1.6094379124341003, 'fanciful': 1.6094379124341003, 'an': 1.6094379124341003, 'odd': 1.6094379124341003, 'idea': 1.6094379124341003, 'in': 1.6094379124341003, 'story': 1.6094379124341003, 'associated': 1.6094379124341003, 'his': 1.6094379124341003, 'is': 1.6094379124341003, 'disaster': 1.6094379124341003, 'disguise': 1.6094379124341003, 'theatrical': 1.6094379124341003, 'about': 1.6094379124341003, 'flying': 1.6094379124341003, 'had': 1.6094379124341003, 'moon': 1.6094379124341003, 'he': 1.6094379124341003, 'a': 1.6094379124341003, 'enjoy': 1.6094379124341003, 'can': 1.6094379124341003, 'be': 1.6094379124341003, 'humorous': 1.6094379124341003, 'someone': 1.6094379124341003, 'time': 1.6094379124341003, 'playing': 1.6094379124341003, 'that': 1.6094379124341003, 'might': 1.6094379124341003, 'she': 1.6094379124341003, 'early': 1.6094379124341003, 'absent': 1.6094379124341003, 'year': 1.6094379124341003, 'been': 1.6094379124341003, 'wa': 1.6094379124341003, 'their': 1.6094379124341003, 'nephew': 1.6094379124341003, 'consciously': 1.6094379124341003, 'one': 1.6094379124341003, 'this': 1.6094379124341003, 'have': 1.6094379124341003, 'summer': 1.6094379124341003, 'addressed': 1.6094379124341003, 'or': 0.9162907318741551, 'of': 0.9162907318741551, 'notion': 0.9162907318741551, 'to': 0.9162907318741551, 'whimsy': 0.9162907318741551, 'it': 0.9162907318741551, 'with': 0.5108256237659907, 'the': 0.5108256237659907}\n",
      " - notion.n.04  overlap: 0.6931  signature:  {'sewing': 0.6931471805599453, 'small': 0.6931471805599453, 'article': 0.6931471805599453, 'clothing': 0.6931471805599453, 'usually': 0.6931471805599453, 'or': 0.6931471805599453, 'item': 0.6931471805599453, 'plural': 0.6931471805599453, 'personal': 0.6931471805599453, 'button': 0.6931471805599453, 'needle': 0.6931471805599453, 'notion': 0.6931471805599453, 'are': 0.6931471805599453, 'and': 0.6931471805599453}\n",
      "------------------------------ \n",
      "\n",
      "# correct synset: notion.n.02\n",
      "# predicted synset naive: impression.n.01 X\n",
      "# predicted synset lesk: impression.n.01 X\n",
      "# predicted synset corpus lesk: notion.n.02 V\n"
     ]
    }
   ],
   "source": [
    "#stampa verbosa di un test\n",
    "token,token_list =get_random_word(data,only_nouns=True)\n",
    "print(\"# word: \",token[\"word\"])\n",
    "print(\"# sentence: \",[t[\"word\"]for t in token_list])\n",
    "print(\"# extracted context: \",get_context(token_list))\n",
    "print(\"# signature of each synset (No corpus examples):\")\n",
    "synsets=wn.synsets(token['word'],pos=token[\"pos\"])\n",
    "for syn in synsets:\n",
    "    signature=get_signature(syn)\n",
    "    overlap=compute_overlap(signature,get_context(token_list))\n",
    "    print(\" -\",syn.name(),\" overlap: \",overlap, \" signature: \",signature)\n",
    "\n",
    "print(\"\\n# signature of each synset (with corpus examples):\")\n",
    "corpus_examples=get_corpus_examples(synsets,data)\n",
    "for syn in synsets:\n",
    "    signature=get_signature(syn,corpus_examples[syn.name()])\n",
    "    overlap=compute_overlap(signature,get_context(token_list),weighted=True)\n",
    "    print(\" -\",syn.name(),\" overlap:\",round(overlap,4), \" signature: \",signature)\n",
    "\n",
    "word = token['word']\n",
    "pos=token['pos']\n",
    "correct_synset = token['syn'].name()\n",
    "def check_prediction(prediction):\n",
    "    return \"V\" if prediction == correct_synset else \"X\"\n",
    "print(\"-\"*30,\"\\n\")\n",
    "print(\"# correct synset:\", correct_synset)\n",
    "print(\"# predicted synset naive:\", (predicted_synset := naive_wsd(word).name()), check_prediction(predicted_synset))\n",
    "print(\"# predicted synset lesk:\", (predicted_synset := lesk(word,pos,token_list).name()), check_prediction(predicted_synset))\n",
    "print(\"# predicted synset corpus lesk:\", (predicted_synset := lesk(word,pos, token_list,data).name()), check_prediction(predicted_synset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Testing\n",
    "Test su 50 frasi casuali, con calcolo di accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prendo tutte le frasse\n",
    "data=[[c for c in s] for s in semcor.tagged_sents(tag='both')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive: 0.7 lesk: 0.64 corpus_lesk: 0.88\n"
     ]
    }
   ],
   "source": [
    "#testa su N frasi naive,lesk e corpus_lesk\n",
    "def testN(data,only_nouns=False,N=50,gemini=False):\n",
    "    c_naive=0\n",
    "    c_lesk=0\n",
    "    c_corpus_lesk=0\n",
    "    c_gemini_lesk=0\n",
    "    for i in range(N):\n",
    "        token,token_list=get_random_word(data,only_nouns)\n",
    "        correct_syn=token['syn']\n",
    "        word=token['word']\n",
    "        pos=token['pos']\n",
    "        if  naive_wsd(word)==correct_syn:\n",
    "            c_naive+=1\n",
    "        if lesk(word,pos,token_list)==correct_syn:\n",
    "            c_lesk+=1\n",
    "        if lesk(word,pos,token_list,data)==correct_syn:\n",
    "            c_corpus_lesk+=1\n",
    "        if gemini:\n",
    "            if lesk(word,pos,token_list,data,gemini)==correct_syn:\n",
    "                c_gemini_lesk+=1\n",
    "        \n",
    "    return c_naive/N,c_lesk/N,c_corpus_lesk/N,c_gemini_lesk/N\n",
    "\n",
    "accuracy=testN(data,only_nouns=True)\n",
    "print(\"naive:\",accuracy[0],\"lesk:\",accuracy[1],\"corpus_lesk:\",accuracy[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0 : 0.42 0.52 0.7\n",
      "Test 1 : 0.34 0.54 0.78\n",
      "Test 2 : 0.3 0.42 0.72\n",
      "Test 3 : 0.42 0.5 0.78\n",
      "Test 4 : 0.36 0.46 0.74\n",
      "Test 5 : 0.48 0.36 0.8\n",
      "Test 6 : 0.34 0.56 0.76\n",
      "Test 7 : 0.38 0.36 0.74\n",
      "Test 8 : 0.52 0.5 0.74\n",
      "Test 9 : 0.38 0.34 0.8\n",
      "----Avarage accuracy: ----\n",
      "naive: 0.394 lesk: 0.456 corpus_lesk: 0.756\n"
     ]
    }
   ],
   "source": [
    "#esegue k=10 volte gli algoritmi di WSD su N=50 frasi, e ne ritorna l'accuracy media\n",
    "def full_test(k=10):\n",
    "    naive_results=0\n",
    "    lesk_results=0\n",
    "    corpus_lesk_results=0\n",
    "    for i in range(k):\n",
    "        n,l,cl,_ =testN(data)\n",
    "        print(\"Test \"+str(i)+\" :\",n,l,cl)\n",
    "        naive_results+=n\n",
    "        lesk_results+=l\n",
    "        corpus_lesk_results+=cl\n",
    "\n",
    "    return(round(naive_results/k,3),round(lesk_results/k,3),round(corpus_lesk_results/k,3))\n",
    "\n",
    "accuracy=full_test() #NON ESEGUIRE IMPIEGA 10 MINUTI\n",
    "print(\"----Avarage accuracy: ----\")\n",
    "print(\"naive:\",accuracy[0],\"lesk:\",accuracy[1],\"corpus_lesk:\",accuracy[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Gemini\n",
    "WSD usando corpus Lesk + esempi aggiuntivi generati da Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "#IMPORTANTE: USARE UNA VPN PER UTILIZZARE GEMINI\n",
    "API_KEY= \"AIzaSyBZKGJMLzRjl7wfLtHLtg2hsNJGEm1V2gg\" #inserire la propria chiave API\n",
    "genai.configure(api_key=API_KEY)\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "gen_config=genai.types.GenerationConfig(temperature=0.7) #gli altri parametri vanno bene di default\n",
    "#spesso i blocchi di sicurezza sono troppo restrittivi, molto spesso non generava esempi, quindi li ho azzerati\n",
    "safe_config = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt engeneering One-shot seguendo il framework COSTAR\n",
    "- (C) Context: \"You are a smart lexicographer who wants to improve the world's best dictionary\"\n",
    "- (O) Obbiettivo: \"Given a definition and some examples for a list of word, write 5 more examples that best represent the meaning of each word. \"\n",
    "- (S) Stile: \"...a friendly, clear style to make the examples sound natural and easy to understand.\"\n",
    "- (T) Tono: \"Use a colloquial tone...\"\n",
    "- (A) Audience: \"For language learners who seek clear and relatable usage of the words.\"\n",
    "- (R) Risposta: \"Write the examples in plain text and insert them into a dictionary in the following format: {\"synset1\": [\"example1\", \"example2\", \"example3\", \"example4\", \"example5\"],\"synset2\": [\"example1\",...],...} [...] Do not use newlines.\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(synsets):\n",
    "    prompt=\"\"\"\n",
    "    You are a smart lexicographer who wants to improve the world's best dictionary. Given a definition and some examples for a list of word, write 5 more examples that best represent the meaning of each word. \n",
    "    Use a colloquial tone and a friendly, clear style to make the examples sound natural and easy to understand for language learners who seek clear and relatable usage of the words\n",
    "    Write the examples in plain text and insert them into a dictionary in the following format: {\"synset1\": [\"example1\", \"example2\", \"example3\", \"example4\", \"example5\"],\"synset2\": [\"example1\",...],...}\n",
    "    Your response will be used as input for a program, so you MUST respect this format. Do not use newlines.\n",
    "\n",
    "    **example of input**\n",
    "    synset= \"bank.n.01\"\n",
    "    Term: \"bank\"\n",
    "    synonyms: \"depository_financial_institution\", \"banking_concern\", \"banking_company\"\n",
    "    Definition: \"a financial institution that accepts deposits and channels the money into lending activities\"\n",
    "    Examples: [\"he cashed a check at the bank\", \"that bank holds the mortgage on my home\"]\n",
    "\n",
    "    **your new 5 examples**\n",
    "    Example of Output: {\"bank.n.01\":[\"she opened a savings account at the local bank\", \"the bank approved his loan for the new car\", \"they went to the bank to deposit their paychecks\", \"the bank's interest rates for loans are very competitive\", \"after losing her debit card, she reported it to the bank immediately\"]}\n",
    "    \n",
    "    **Input:**\n",
    "    \"\"\"\n",
    "    for syn in synsets:\n",
    "        prompt+=\"\"\"\n",
    "        Term: \"\"\" + syn.lemmas()[0].name()+ \"\"\"  \n",
    "        synonyms: \"\"\" + str([lemma.name() for lemma in syn.lemmas()[1:]]) + \"\"\"\n",
    "        Definition: \"\"\" + str(syn.definition()) + \"\"\"\n",
    "        Examples: \"\"\" + str(syn.examples()) + \"\"\"\n",
    "        \"\"\"\n",
    "    prompt+=\"\"\"\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#data una lista di synset ritorna un dizionario, in cui ad ogni synset è associata una lita di esempi di utilizzo generate con gemini\n",
    "def get_gemini_examples(synsets):\n",
    "    prompt=get_prompt(synsets)\n",
    "    #chiamata API gemini\n",
    "    response = model.generate_content(prompt,generation_config=gen_config,safety_settings=safe_config)\n",
    "    time.sleep(1)#per evitare di superare il limite di richieste\n",
    "    #se la generazione non è andata a buon fine (es safety reasons riprovo massimo 2 volte)\n",
    "    if response.candidates[0].finish_reason>1  :\n",
    "        for i in range(2):\n",
    "            print(\"Error, finish reason: \",response.candidates[0].finish_reason)\n",
    "            response = model.generate_content(prompt,generation_config=gen_config)\n",
    "            if response.candidates[0].finish_reason==1:\n",
    "                break\n",
    "        return None\n",
    "    #controllo che la stringa ottenuta sia convertibile in lista\n",
    "    try:\n",
    "        example_list = eval(response.text)\n",
    "        return example_list\n",
    "    except:\n",
    "        print(\"Error, output not formatted correctly\")\n",
    "        print(response.text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L'accuracy di corpus-lesk con anche l'utilizzo di esempi generati da Gemini sono equivalenti a quelle di corpus-lesk con solo gli esempi di Semcor.\n",
    "- Confrontando le singature ottenute solo con Semcor e quelle con Semcor+Gemini, notiamo leggerissime differenze, probabilmente non sufficienti per un incremento sensibile nelle performance.\n",
    "- Se vengono utilizzati solo gli esempi di Gemini le prestazioni crollano, evidentemente non sono sufficienti per disambiguare correttamente i sensi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive: 0.4 lesk: 0.6 corpus_lesk: 0.8 gemini_corpus_lesk: 0.8\n"
     ]
    }
   ],
   "source": [
    "#Test di utilizzo \n",
    "accuracy=testN(data,only_nouns=False,N=5,gemini=True)\n",
    "print(\"naive:\",accuracy[0],\"lesk:\",accuracy[1],\"corpus_lesk:\",accuracy[2],\"gemini_corpus_lesk:\",accuracy[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TLNenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
