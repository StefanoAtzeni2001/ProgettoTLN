{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sense Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "synset1 = wn.synsets('cat')[0]\n",
    "synset2 = wn.synsets('dog')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcola la profondità di un synset, prendendo il percorso più lungo tra i suoi iperonimi\n",
    "def depth(synset):\n",
    "    hypernym_paths = synset.hypernym_paths()\n",
    "    max_depth = max(len(path) for path in hypernym_paths)\n",
    "    return max_depth\n",
    "\n",
    "#calcola il lowest common hypernym tra due synset e ne calcola la profondità\n",
    "def lcs_depth(synset1, synset2):\n",
    "    lcs = synset1.lowest_common_hypernyms(synset2)\n",
    "    if lcs: return depth(lcs[0])\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wu-Palmer\n",
    "\n",
    "### $sim_{wup}(s_1,s_2)=\\frac{2*\\text{depth(LCS)}}{\\text{depth}(s_1)+\\text{depth}(s_2)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "#implementazione metrica di similarità di Wu-Palmer\n",
    "def wup_similarity(synset1, synset2):\n",
    "    depthLCS = lcs_depth(synset1, synset2)\n",
    "    depth1 = depth(synset1)\n",
    "    depth2 = depth(synset2)\n",
    "    similarity = (2 * depthLCS) / (depth1 + depth2)\n",
    "    return similarity\n",
    "\n",
    "print(wup_similarity(synset1,synset2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortest Path\n",
    "\n",
    "### $sim_{path}(s_1,s_2)=2*\\text{depthMax}-\\text{len}(s_1,s_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Max depth=20\n",
    "max(max(len(hyp_path) for hyp_path in ss.hypernym_paths()) for ss in wn.all_synsets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "#implementazione metrica di similarità di Shortest Path\n",
    "def path_similarity(synset1, synset2):\n",
    "    len=synset1.shortest_path_distance(synset2) \n",
    "    if len is None: return 0\n",
    "    return (2*20-len)\n",
    "print(path_similarity(synset1,synset2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leakcook - Chodorow\n",
    "\n",
    "### $sim_{LC}(s_1,s_2)=-log\\frac{\\text{len}(s_1,s_2)+1}{2*\\text{depthMax}+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1041341542702074\n"
     ]
    }
   ],
   "source": [
    "#implementazione metrica di similarità di Leacock-Chodorow\n",
    "def lc_similarity(synset1, synset2):\n",
    "    len=synset1.shortest_path_distance(synset2) \n",
    "    if len is None: return 0\n",
    "    return -np.log((len+1)/((2*20)+1))\n",
    "\n",
    "print(lc_similarity(synset1,synset2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu Palmer: 0.8571428571428571\n",
      "Shortest Path: 36\n",
      "Leakcoock Chodorow 2.1041341542702074\n"
     ]
    }
   ],
   "source": [
    "#per calcolare la similarità tra due termini,\n",
    "#prendo la similarità maggiore tra tutte le combinazioni di synset dei due termini\n",
    "def term_similarity(term1,term2,metric):\n",
    "    synsets1 = wn.synsets(term1)\n",
    "    synsets2 = wn.synsets(term2)\n",
    "    best_similarity=0\n",
    "    for s1 in synsets1:\n",
    "        for s2 in synsets2:\n",
    "            similarity = metric(s1,s2)\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "    return best_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term1</th>\n",
       "      <th>term2</th>\n",
       "      <th>human_sim</th>\n",
       "      <th>wup_sim</th>\n",
       "      <th>path_sim</th>\n",
       "      <th>lc_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>39</td>\n",
       "      <td>3.020425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>39</td>\n",
       "      <td>3.020425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>3.713572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>38</td>\n",
       "      <td>2.614960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>37</td>\n",
       "      <td>2.327278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>shower</td>\n",
       "      <td>flood</td>\n",
       "      <td>6.03</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>36</td>\n",
       "      <td>2.104134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>weather</td>\n",
       "      <td>forecast</td>\n",
       "      <td>8.34</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>27</td>\n",
       "      <td>1.074515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>disaster</td>\n",
       "      <td>area</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>32</td>\n",
       "      <td>1.516347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>governor</td>\n",
       "      <td>office</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>31</td>\n",
       "      <td>1.410987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>architecture</td>\n",
       "      <td>century</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>31</td>\n",
       "      <td>1.410987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            term1     term2  human_sim   wup_sim  path_sim    lc_sim\n",
       "0            love       sex       6.77  0.923077        39  3.020425\n",
       "1           tiger       cat       7.35  0.965517        39  3.020425\n",
       "2           tiger     tiger      10.00  1.000000        40  3.713572\n",
       "3            book     paper       7.46  0.875000        38  2.614960\n",
       "4        computer  keyboard       7.62  0.823529        37  2.327278\n",
       "..            ...       ...        ...       ...       ...       ...\n",
       "348        shower     flood       6.03  0.636364        36  2.104134\n",
       "349       weather  forecast       8.34  0.133333        27  1.074515\n",
       "350      disaster      area       6.25  0.500000        32  1.516347\n",
       "351      governor    office       6.34  0.526316        31  1.410987\n",
       "352  architecture   century       3.78  0.307692        31  1.410987\n",
       "\n",
       "[353 rows x 6 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sim_df = pd.read_csv('../data/WordSim353.csv')\n",
    "\n",
    "wup_sim = []\n",
    "path_sim = []\n",
    "lc_sim = []\n",
    "for index, row in sim_df.iterrows():\n",
    "    wup_sim.append(term_similarity(row['term1'], row['term2'],wup_similarity))\n",
    "    path_sim.append(term_similarity(row['term1'], row['term2'],path_similarity))\n",
    "    lc_sim.append(term_similarity(row['term1'], row['term2'],lc_similarity))\n",
    "\n",
    "sim_df['wup_sim'] = wup_sim\n",
    "sim_df['path_sim'] = path_sim\n",
    "sim_df['lc_sim'] = lc_sim\n",
    "sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(X, Y):\n",
    "    mean_X = np.mean(X)\n",
    "    mean_Y = np.mean(Y)\n",
    "    cov = np.mean((X - mean_X) * (Y - mean_Y))\n",
    "    std_X = np.std(X, ddof=1)\n",
    "    std_Y = np.std(Y, ddof=1)\n",
    "    pearson_corr = cov / (std_X * std_Y)\n",
    "    return pearson_corr\n",
    "\n",
    "def rank_data(X):\n",
    "    sorted_X = sorted([(value, index) for index, value in enumerate(X)])\n",
    "    ranks = [0] * len(X)\n",
    "    for i, (_, index) in enumerate(sorted_X):\n",
    "        ranks[index] = i + 1\n",
    "    return ranks\n",
    "\n",
    "def spearman_correlation(X, Y):\n",
    "    X_rank = rank_data(X)\n",
    "    Y_rank = rank_data(Y)\n",
    "    pearson_corr_rank = pearson_correlation(X_rank, Y_rank)\n",
    "    return pearson_corr_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person correlation\n",
      "wup_sim:  0.2865121430303598\n",
      "path_sim:  0.16606040518129\n",
      "lc_sim:  0.31346970268590574\n",
      "\n",
      "Spearmann correlation\n",
      "wup_sim:  0.3443780960403464\n",
      "path_sim:  0.2846033239430566\n",
      "lc_sim:  0.2846033239430566\n"
     ]
    }
   ],
   "source": [
    "print(\"Person correlation\")\n",
    "print(\"wup_sim: \",pearson_correlation(sim_df['human_sim'], sim_df['wup_sim']))\n",
    "print(\"path_sim: \",pearson_correlation(sim_df['human_sim'], sim_df['path_sim']))\n",
    "print(\"lc_sim: \",pearson_correlation(sim_df['human_sim'], sim_df['lc_sim']))\n",
    "\n",
    "print(\"\\nSpearmann correlation\")\n",
    "print(\"wup_sim: \",spearman_correlation(sim_df['human_sim'], sim_df['wup_sim']))\n",
    "print(\"path_sim: \",spearman_correlation(sim_df['human_sim'], sim_df['path_sim']))\n",
    "print(\"lc_sim: \",spearman_correlation(sim_df['human_sim'], sim_df['lc_sim']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TLNenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
