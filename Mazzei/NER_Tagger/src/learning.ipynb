{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         POSITION      WORD TAG\n",
      "0               0      This   O\n",
      "1               1  division   O\n",
      "2               2      also   O\n",
      "3               3  contains   O\n",
      "4               4       the   O\n",
      "...           ...       ...  ..\n",
      "2193674        27      born   O\n",
      "2193675        28         1   O\n",
      "2193676        29       May   O\n",
      "2193677        30      1964   O\n",
      "2193678        31         .   O\n",
      "\n",
      "[2193679 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "# function to read data from file\n",
    "def read_dataset(file_name,language):\n",
    "    path=\"../data/\"+language+\"/\"+file_name+\".conllu\"\n",
    "    data = pd.read_csv (path, sep = '\\t',quoting=3, names=[\"POSITION\",\"WORD\",\"TAG\"])\n",
    "    return data\n",
    "\n",
    "train_data=read_dataset(\"train\",\"en\")\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNING:\n",
    "### Counting\n",
    "- word_tag_counts = number of times a word is associated with each tag\n",
    "- tag_tag_counts =  number of times a tag is followed by another tag    (also considering an extra tag \"START\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': {'O': 1692935, 'B-LOC': 52617, 'I-LOC': 0, 'B-ORG': 23152, 'B-MISC': 34187, 'I-MISC': 3831, 'B-PER': 36663, 'I-PER': 0, 'I-ORG': 0}, 'B-LOC': {'O': 40581, 'B-LOC': 134, 'I-LOC': 14174, 'B-ORG': 0, 'B-MISC': 0, 'I-MISC': 0, 'B-PER': 10, 'I-PER': 3, 'I-ORG': 0}, 'I-LOC': {'O': 14163, 'B-LOC': 4, 'I-LOC': 4592, 'B-ORG': 1, 'B-MISC': 1, 'I-MISC': 0, 'B-PER': 6, 'I-PER': 0, 'I-ORG': 0}, 'B-ORG': {'O': 12196, 'B-LOC': 0, 'I-LOC': 0, 'B-ORG': 52, 'B-MISC': 0, 'I-MISC': 0, 'B-PER': 11, 'I-PER': 0, 'I-ORG': 11966}, 'B-MISC': {'O': 23679, 'B-LOC': 0, 'I-LOC': 0, 'B-ORG': 0, 'B-MISC': 821, 'I-MISC': 11644, 'B-PER': 18, 'I-PER': 1, 'I-ORG': 0}, 'I-MISC': {'O': 15455, 'B-LOC': 0, 'I-LOC': 1, 'B-ORG': 0, 'B-MISC': 17, 'I-MISC': 17915, 'B-PER': 0, 'I-PER': 2, 'I-ORG': 1}, 'B-PER': {'O': 14419, 'B-LOC': 0, 'I-LOC': 0, 'B-ORG': 0, 'B-MISC': 0, 'I-MISC': 0, 'B-PER': 14, 'I-PER': 25831, 'I-ORG': 0}, 'I-PER': {'O': 25833, 'B-LOC': 0, 'I-LOC': 0, 'B-ORG': 0, 'B-MISC': 0, 'I-MISC': 1, 'B-PER': 3, 'I-PER': 3629, 'I-ORG': 0}, 'I-ORG': {'O': 11954, 'B-LOC': 0, 'I-LOC': 0, 'B-ORG': 8, 'B-MISC': 0, 'I-MISC': 0, 'B-PER': 5, 'I-PER': 0, 'I-ORG': 8429}}\n"
     ]
    }
   ],
   "source": [
    "def counts_occurence(train_data):\n",
    "  tag_list=train_data['TAG'].unique()\n",
    "  word_list=train_data['WORD'].unique()\n",
    "  empty_tag_count_dict= {tag:0 for tag in tag_list}\n",
    "\n",
    "  tag_counts = empty_tag_count_dict.copy()  # {tag1:0, tag2:0, ...}\n",
    "  word_tag_counts = {word:empty_tag_count_dict.copy() for word in word_list} # {word1:{tag1:0, tag2:0, ...}, word2:{tag1:0, tag2:0, ...}, ...}\n",
    "  tag_tag_counts = {tag:empty_tag_count_dict.copy() for tag in tag_list} # {tag1:{tag1:0, tag2:0, ...}, tag2:{tag1:0, tag2:0, ...}, ...}\n",
    "  #tag_tag_counts['START'] = empty_tag_count_dict.copy()\n",
    "  #tag_counts['START'] = 0\n",
    " \n",
    "\n",
    "  for index, row in train_data.iterrows():\n",
    "      word = row['WORD']\n",
    "      tag = row['TAG']\n",
    "      pos= row['POSITION']\n",
    "      \n",
    "      if pos!=0:\n",
    "        tag_tag_counts[prev_tag][tag] +=1\n",
    "\n",
    "      \n",
    "      tag_counts[tag] += 1\n",
    "      word_tag_counts[word][tag] +=1\n",
    "      prev_tag = tag\n",
    "\n",
    "  \n",
    "  return tag_counts, word_tag_counts, tag_tag_counts\n",
    "\n",
    "tag_counts, word_tag_counts, tag_tag_counts= counts_occurence(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities\n",
    "- emission_prob = probability, given a tag, that it will be associated with a given word\n",
    "- transition_prob = probability of a tag occurring given the previous tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probs(tag_counts, word_tag_counts, tag_tag_counts):\n",
    "    tag_list=train_data['TAG'].unique()\n",
    "    word_list=train_data['WORD'].unique()\n",
    "    empty_tag_count_dict= {tag:0 for tag in tag_list}\n",
    "\n",
    "    emission_prob = {word:empty_tag_count_dict.copy() for word in word_list} \n",
    "    transition_prob= {tag:empty_tag_count_dict.copy() for tag in tag_list} \n",
    "    #transition_prob['START'] = empty_tag_count_dict.copy()\n",
    "\n",
    "    for word, tag_dict in word_tag_counts.items():\n",
    "        for tag in tag_dict:\n",
    "            if word_tag_counts[word][tag]!=0:\n",
    "                emission_prob[word][tag] = np.log(word_tag_counts[word][tag]/tag_counts[tag])                    \n",
    "            else:\n",
    "                emission_prob[word][tag] = 0\n",
    "        \n",
    "    for prev, tag_dict in tag_tag_counts.items():\n",
    "        for next in tag_dict:\n",
    "            if tag_tag_counts[prev][next]!=0:\n",
    "                transition_prob[prev][next] = np.log(tag_tag_counts[prev][next]/tag_counts[prev])\n",
    "            else:\n",
    "                transition_prob[prev][next] = 0\n",
    "    return emission_prob, transition_prob\n",
    "    \n",
    "emission_prob, transition_prob= calculate_probs(tag_counts, word_tag_counts, tag_tag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             This  division      also  contains        the    Ventana  \\\n",
      "O       -6.732052 -9.069017 -5.812474 -9.016603  -2.777174   0.000000   \n",
      "B-LOC    0.000000  0.000000  0.000000  0.000000  -8.428398 -10.220158   \n",
      "I-LOC    0.000000  0.000000  0.000000  0.000000  -5.796804   0.000000   \n",
      "B-ORG    0.000000  0.000000  0.000000  0.000000  -5.875633   0.000000   \n",
      "B-MISC  -7.930842  0.000000  0.000000  0.000000  -8.097897   0.000000   \n",
      "I-MISC  -8.336600  0.000000  0.000000  0.000000  -3.529510   0.000000   \n",
      "B-PER  -10.603213  0.000000  0.000000  0.000000 -10.603213   0.000000   \n",
      "I-PER  -10.290992  0.000000  0.000000  0.000000  -5.423458   0.000000   \n",
      "I-ORG    0.000000  0.000000  0.000000  0.000000  -4.817149   0.000000   \n",
      "\n",
      "        Wilderness         ,      home        to  ...       Rapp  shanklish  \\\n",
      "O         0.000000 -2.836472 -7.562451 -3.932190  ...   0.000000 -14.476189   \n",
      "B-LOC     0.000000  0.000000  0.000000  0.000000  ...   0.000000   0.000000   \n",
      "I-LOC    -7.537270 -9.839855  0.000000  0.000000  ...   0.000000   0.000000   \n",
      "B-ORG     0.000000  0.000000  0.000000  0.000000  ...   0.000000   0.000000   \n",
      "B-MISC    0.000000  0.000000  0.000000  0.000000  ...   0.000000   0.000000   \n",
      "I-MISC   -9.317429 -5.280243  0.000000 -5.179600  ...   0.000000   0.000000   \n",
      "B-PER     0.000000  0.000000  0.000000  0.000000  ...   0.000000   0.000000   \n",
      "I-PER     0.000000 -7.726043  0.000000  0.000000  ... -10.290992   0.000000   \n",
      "I-ORG    -9.923094 -6.927362  0.000000 -7.977184  ...   0.000000   0.000000   \n",
      "\n",
      "           Areesh  Tomomichi  Nishimura      Nobuo     Tobita   Hewitson  \\\n",
      "O        0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "B-LOC    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "I-LOC    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "B-ORG    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "B-MISC   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "I-MISC   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "B-PER  -10.603213 -10.603213   0.000000 -10.603213   0.000000   0.000000   \n",
      "I-PER    0.000000   0.000000 -10.290992   0.000000 -10.290992 -10.290992   \n",
      "I-ORG    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "\n",
      "           Clynes  Caesarean  \n",
      "O        0.000000   0.000000  \n",
      "B-LOC    0.000000   0.000000  \n",
      "I-LOC    0.000000   0.000000  \n",
      "B-ORG    0.000000   0.000000  \n",
      "B-MISC   0.000000 -10.495792  \n",
      "I-MISC   0.000000   0.000000  \n",
      "B-PER    0.000000   0.000000  \n",
      "I-PER  -10.290992   0.000000  \n",
      "I-ORG    0.000000   0.000000  \n",
      "\n",
      "[9 rows x 111342 columns]\n"
     ]
    }
   ],
   "source": [
    "###CODICE TEST DA BUTTARE,PER VEDERE LA SOMMA DELLE PROBABILITÀ: non è esattamente uno a causa di approssimazioni, spero non crei problemi\n",
    "\n",
    "emission_dataframe=pd.DataFrame.from_dict(emission_prob)\n",
    "transition_dataframe=pd.DataFrame.from_dict(transition_prob)\n",
    "\n",
    "print(emission_dataframe)\n",
    "#tag_sums1 = {}\n",
    "#tag_sums2 = {}\n",
    "#for word in emission_prob:\n",
    "#    for tag, value in emission_prob[word].items():\n",
    "#        tag_sums1[tag] = tag_sums1.get(tag, 0) + value\n",
    "#    print(\"emission\",tag_sums1)\n",
    "\n",
    "#for tag in transition_prob:\n",
    "#     for next_tag, value in transition_prob[tag].items():\n",
    "#         tag_sums2[tag] = tag_sums2.get(tag, 0) + value\n",
    "#     print(\"transition\",tag_sums2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#serialize data into a file \n",
    "def save_data(data,file_name,language):\n",
    "    path=\"../data/\"+language+\"/\"+file_name\n",
    "    try: \n",
    "        file= open(path, 'wb') \n",
    "        pickle.dump(data, file) \n",
    "        file.close() \n",
    "    except: \n",
    "        print(\"Error in writing data\")\n",
    "\n",
    "# save_data(transition_prob,\"transition_prob\",\"it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learn emission and transition probabilities for each language\n",
    "for language in [\"en\",\"it\",\"es\"]:\n",
    "    read_dataset(\"train\",language)\n",
    "    tag_counts, word_tag_counts, tag_tag_counts= counts_occurence(train_data)\n",
    "    emission_prob, transition_prob= calculate_probs(tag_counts, word_tag_counts, tag_tag_counts)\n",
    "    save_data(transition_prob,\"transition_prob\",language)\n",
    "    save_data(emission_prob,\"emission_prob\",language)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAUTenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
