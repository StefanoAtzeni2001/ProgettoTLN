{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzioni di Gestione files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from conllu import parse, TokenList\n",
    "\n",
    "#deserialize data from a file\n",
    "def load_data(file_name,language):\n",
    "    path=\"../data/\"+language+\"/\"+file_name\n",
    "    try: \n",
    "        file = open(path, 'rb') \n",
    "        data = pickle.load(file) \n",
    "        return data\n",
    "    except: \n",
    "        print(\"Error in reading data\")\n",
    "\n",
    "\n",
    "# function to read data from file\n",
    "def read_dataset(file_name,language):\n",
    "    path=\"../data/\"+language+\"/dataset/\"+file_name+\".conllu\"\n",
    "    data = pd.read_csv (path, sep = '\\t',quoting=3, names=[\"POSITION\",\"WORD\",\"TAG\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Funzione per salvare il DataFrame in un file CoNLL-U\n",
    "def save_to_conllu(dataframe,file_name,language):\n",
    "    # Creazione della lista di token da DataFrame\n",
    "    path=\"../data/\"+language+\"/tagging/\"+file_name\n",
    "    tokens = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        token = {\n",
    "            \"id\": row['POSITION'],\n",
    "            \"form\": row['WORD'],\n",
    "            \"misc\":  row['TAG']\n",
    "        }\n",
    "        tokens.append(token)\n",
    "    \n",
    "    # Creazione dell'oggetto TokenList\n",
    "    token_list = TokenList(tokens)\n",
    "    \n",
    "    # Scrittura del TokenList nel file CoNLL-U\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(token_list.serialize())\n",
    "    print(\"DataFrame salvato in formato CoNLL-U:\", path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzioni di manipolazione e creazione del golden system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_dataframe(df):\n",
    "    sentences = ''\n",
    "    for index, row in df.iterrows():\n",
    "        word = row['WORD']\n",
    "        if pd.notnull(word):  # Se la parola non è nulla\n",
    "            if sentences:  # Se c'è già una frase, aggiungi uno spazio prima della nuova parola\n",
    "                sentences += ' '\n",
    "            sentences += word\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "def create_golden_dataframe(sentence, golden_tag):\n",
    "    # Dividi la frase e i tag dorati in parole e tag\n",
    "    words = sentence.split()\n",
    "    tags = golden_tag.split(',')\n",
    "    \n",
    "    # Controlla se il numero di parole e tag è lo stesso\n",
    "    if len(words) != len(tags):\n",
    "        raise ValueError(\"Il numero di parole e tag dorati non corrisponde.\")\n",
    "\n",
    "    # Creazione del DataFrame dorato\n",
    "    df = pd.DataFrame({'WORD': words, 'TAG': tags})\n",
    "    df['POSITION'] = df.index + 1  # Aggiunge la colonna POSITION\n",
    "    df = df[['POSITION', 'WORD', 'TAG']]  # Riordina le colonne\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementazione dell'algoritmo di viterbi con la prevenzione dell'underflow tramite logaritmo e probabilità iniziale omogenea\n",
    "\n",
    "def viterbi(emission_df, transition_df):\n",
    "    # Numero di stati\n",
    "    num_states = len(transition_df)\n",
    "\n",
    "    # Inizializzazione della matrice di probabilità\n",
    "    dp = pd.DataFrame(index=range(num_states), columns=range(len(emission_df.columns)))\n",
    "    dp.iloc[:, 0] = np.log(1 / num_states) + np.log(emission_df.iloc[:, 0] + 1e-10)\n",
    "\n",
    "    # Inizializzazione del percorso ottimale\n",
    "    path = {state: [state] for state in range(num_states)}\n",
    "\n",
    "    # Ciclo attraverso le osservazioni\n",
    "    for t in range(1, len(emission_df.columns)):\n",
    "        new_path = {}\n",
    "\n",
    "        # Ciclo attraverso i possibili stati\n",
    "        for state in range(num_states):\n",
    "            # Calcolo della probabilità massima\n",
    "            max_prob = float('-inf')\n",
    "            max_state = None\n",
    "            for prev_state in range(num_states):\n",
    "                prob = dp.iloc[prev_state, t-1] + np.log(transition_df.iloc[prev_state, state] + 1e-10) + np.log(emission_df.iloc[state, t] + 1e-10)\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_state = prev_state\n",
    "            \n",
    "            dp.iloc[state, t] = max_prob\n",
    "\n",
    "            # Aggiornamento del percorso ottimale\n",
    "            new_path[state] = path[max_state] + [state]\n",
    "\n",
    "        path = new_path\n",
    "\n",
    "    # Ritorno del percorso ottimale\n",
    "    max_prob = dp.iloc[:, len(emission_df.columns)-1].max()\n",
    "    max_path = path[dp.iloc[:, len(emission_df.columns)-1].idxmax()]\n",
    "\n",
    "    # Stampa a schermo il percorso di Viterbi\n",
    "    print('Il percorso di Viterbi è:', ' -> '.join(emission_df.index[max_path]))\n",
    "\n",
    "    return pd.DataFrame({'POSITION': range(len(max_path)), 'WORD': emission_df.columns, 'TAG': [emission_df.index[state] for state in max_path]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione del sub-dataset di probabilità di emissione per le parole di una frase.\n",
    "\n",
    "Applicazione di diverse tecniche di smoothing per gestire le parole sconosciute:\n",
    "\n",
    "1 - Sempre O: P(unk|O) = 1\n",
    "\n",
    "2 - Sempre O o MISC: P(unk|O)=P(unk|B-MISC)=0.5\n",
    "\n",
    "3 - Uniforme: P(unk|tag) = 1/#(NER_TAGs)\n",
    "\n",
    "4 - Statistica TAG sul val set: parole che compaiono 1 sola volta  -> unknown_prob calcolata nel file learning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prende in input una frase, le probabilità di emisione e transizione apprese \n",
    "#restituisce le coppie parola-NER_TAG assegnate utilizzano l'algoritmo di Viterbi e applicando la tecnica di smoothing specificata\n",
    "def viterbi_tagger(sentence, emission_prob, transition_prob, unkown_prob, smoothing_type=1):\n",
    "    #inizializzazione\n",
    "    tags=transition_prob.keys()\n",
    "    words = sentence.split()\n",
    "    transition_df = pd.DataFrame.from_dict(transition_prob)\n",
    "    emission_sentence_df = pd.DataFrame(columns=words,index=tags)\n",
    "    \n",
    "    #iterazione per ogni parola delle frase aggiorna il dataframe delle emissioni\n",
    "    for word in words:\n",
    "        if word in emission_prob:\n",
    "            emission_sentence_df[word] = pd.Series(emission_prob[word]).values\n",
    "\n",
    "\n",
    "        else: #applicazione dello smoothing\n",
    "           if (smoothing_type==1): emission_sentence_df[word] =  {tag: 1 if tag == \"O\" else 0 for tag in tags}\n",
    "           elif (smoothing_type==2): emission_sentence_df[word] =  {tag: 0.5 if tag == \"B-MISC\" or tag == \"O\" else 0.01 for tag in tags}\n",
    "           elif (smoothing_type==3): emission_sentence_df[word] =  {tag: 1/len(tags) for tag in tags}\n",
    "           elif (smoothing_type==4): emission_sentence_df[word] =  unkown_prob\n",
    "   \n",
    "    return viterbi(emission_sentence_df, transition_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVE TAGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive tagger --> utilizza la probabilità di emissione più alta, se parola sconosciuta --> B-MISC\n",
    "\n",
    "def naive_tagger(sentence, emission_prob):\n",
    "    tags = []\n",
    "    words = sentence.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in emission_prob:\n",
    "            tags.append(max(emission_prob[word], key=emission_prob[word].get))\n",
    "        else:\n",
    "            tags.append(\"B-MISC\")\n",
    "    \n",
    "    # Creazione del DataFrame\n",
    "    df = pd.DataFrame({'WORD': words, 'TAG': tags})\n",
    "    df['POSITION'] = df.index + 1  # Aggiunge la colonna POSITION\n",
    "    df = df[['POSITION', 'WORD', 'TAG']]  # Riordina le colonne\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esempio di Decoding Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m      9\u001b[0m sentence \u001b[38;5;241m=\u001b[39m extract_sentences_from_dataframe(golden_df)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#Possibilità di effettuare tagging di una stringa in input\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#sentence =\"Il Vermont non era ancora stato colonizzato , mentre i territori del New Hampshire e del Mane erano governati dal Massachusetts .\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#golden_tag = \"O,B-LOC,O,O,O,O,O,O,O,O,O,O,B-LOC,I-LOC,O,O,B-LOC,O,O,O,B-LOC,O\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#golden_df = create_golden_dataframe(sentence, golden_tag)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m vit_df\u001b[38;5;241m=\u001b[39m\u001b[43mviterbi_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43memission_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtransition_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43munkown_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m nayve_df\u001b[38;5;241m=\u001b[39mnaive_tagger(sentence, emission_prob)\n\u001b[0;32m     21\u001b[0m save_to_conllu(golden_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgolden_tag.conllu\u001b[39m\u001b[38;5;124m\"\u001b[39m, language)\n",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m, in \u001b[0;36mviterbi_tagger\u001b[1;34m(sentence, emission_prob, transition_prob, unkown_prob, smoothing_type)\u001b[0m\n\u001b[0;32m      6\u001b[0m words \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      7\u001b[0m transition_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(transition_prob)\n\u001b[1;32m----> 8\u001b[0m emission_sentence_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#iterazione per ogni parola delle frase aggiorna il dataframe delle emissioni\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:876\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    867\u001b[0m             mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    868\u001b[0m                 data,\n\u001b[0;32m    869\u001b[0m                 index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    873\u001b[0m                 typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    874\u001b[0m             )\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 876\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdefault_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;66;03m# For data is scalar\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:462\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m midxs:\n\u001b[0;32m    461\u001b[0m         arr \u001b[38;5;241m=\u001b[39m sanitize_array(arrays\u001b[38;5;241m.\u001b[39miat[i], index, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m--> 462\u001b[0m         \u001b[43marrays\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# GH#1783\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     nan_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexing.py:2542\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough indexers for scalar access (setting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2542\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:1447\u001b[0m, in \u001b[0;36mSeries._set_value\u001b[1;34m(self, label, value, takeable)\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1445\u001b[0m     loc \u001b[38;5;241m=\u001b[39m label\n\u001b[1;32m-> 1447\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:1419\u001b[0m, in \u001b[0;36mSeries._set_values\u001b[1;34m(self, key, value, warn)\u001b[0m\n\u001b[0;32m   1416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Index, Series)):\n\u001b[0;32m   1417\u001b[0m     key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_update_cacher()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:415\u001b[0m, in \u001b[0;36mBaseBlockManager.setitem\u001b[1;34m(self, indexer, value, warn)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;66;03m# No need to split if we either set all columns or on a single block\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;66;03m# manager\u001b[39;00m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msetitem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\blocks.py:1418\u001b[0m, in \u001b[0;36mBlock.setitem\u001b[1;34m(self, indexer, value, using_cow)\u001b[0m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m _dtype_obj:\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;66;03m# TODO: avoid having to construct values[indexer]\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m     vi \u001b[38;5;241m=\u001b[39m values[indexer]\n\u001b[1;32m-> 1418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvi\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1419\u001b[0m         \u001b[38;5;66;03m# checking lib.is_scalar here fails on\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m         \u001b[38;5;66;03m#  test_iloc_setitem_custom_object\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m         casted \u001b[38;5;241m=\u001b[39m setitem_datetimelike_compat(values, \u001b[38;5;28mlen\u001b[39m(vi), casted)\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_copy(using_cow, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for language in [\"en\",\"it\",\"es\"]: \n",
    " emission_prob=load_data(\"emission_prob\",language)\n",
    " transition_prob=load_data(\"transition_prob\",language)\n",
    " unkown_prob=load_data(\"unknown_prob\",language)\n",
    "\n",
    " #carico il file di test, estraggo la sentence e pongo il test set df come golden_df\n",
    " golden_tot = read_dataset(\"test\",language)\n",
    " golden_df = golden_tot\n",
    " sentence = extract_sentences_from_dataframe(golden_df)\n",
    "\n",
    "\n",
    " #Possibilità di effettuare tagging di una stringa in input\n",
    " #sentence =\"Il Vermont non era ancora stato colonizzato , mentre i territori del New Hampshire e del Mane erano governati dal Massachusetts .\"\n",
    " #golden_tag = \"O,B-LOC,O,O,O,O,O,O,O,O,O,O,B-LOC,I-LOC,O,O,B-LOC,O,O,O,B-LOC,O\"\n",
    " #golden_df = create_golden_dataframe(sentence, golden_tag)\n",
    "\n",
    " vit_df=viterbi_tagger(sentence,emission_prob,transition_prob,unkown_prob,1)\n",
    " nayve_df=naive_tagger(sentence, emission_prob)\n",
    "\n",
    "\n",
    " save_to_conllu(golden_df, \"golden_tag.conllu\", language)\n",
    " save_to_conllu(vit_df, \"viterbi_tag.conllu\", language)\n",
    " save_to_conllu(nayve_df, \"nayve_tag.conllu\", language)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TLNenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
