{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# function to read data from file\n",
    "def read_tagging(file_name,language):\n",
    "    path=\"../data/\"+language+\"/tagging/\"+file_name+\".conllu\"\n",
    "    data = pd.read_csv (path, sep = '\\t',quoting=3, names=[\"POSITION\",\"WORD\",\"TAG\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_sentences_from_dataframe(df):\n",
    "    sentences = ''\n",
    "    for index, row in df.iterrows():\n",
    "        word = row['WORD']\n",
    "        if pd.notnull(word):  # Se la parola non è nulla\n",
    "            if sentences:  # Se c'è già una frase, aggiungi uno spazio prima della nuova parola\n",
    "                sentences += ' '\n",
    "            sentences += word\n",
    "    return sentences\n",
    "\n",
    "#deserialize data from a file\n",
    "def load_data(file_name,language):\n",
    "    path=\"../data/\"+language+\"/\"+file_name\n",
    "    try: \n",
    "        file = open(path, 'rb') \n",
    "        data = pickle.load(file) \n",
    "        return data\n",
    "    except: \n",
    "        print(\"Error in reading data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_entities_from_dataframe(dataframe):\n",
    "    entity_spans = []\n",
    "\n",
    "    current_entity_span = None\n",
    "    current_sentence_index = 0\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        word = row['WORD']\n",
    "        tag = row['TAG']\n",
    "        position = row['POSITION']\n",
    "\n",
    "        if position == 0:  # Inizio di una nuova frase\n",
    "            current_sentence_index += 1\n",
    "\n",
    "        if tag != 'O':\n",
    "            if tag.startswith('B-'):\n",
    "                # Se inizia una nuova entità, chiudi quella corrente e inizia una nuova\n",
    "                if current_entity_span is not None:\n",
    "                    entity_spans.append(current_entity_span)\n",
    "                current_entity_span = {'Tag': tag[2:], 'Sentence Number': current_sentence_index}\n",
    "                current_entity_span['Start Index'] = index\n",
    "                current_entity_span['End Index'] = index\n",
    "            elif tag.startswith('I-'):\n",
    "                # Aggiungi la parola all'entità corrente\n",
    "                if current_entity_span is not None:\n",
    "                    current_entity_span['End Index'] = index\n",
    "            else:\n",
    "                print(\"Errore: Tag non riconosciuto.\")\n",
    "\n",
    "        else:\n",
    "            # Se il tag è \"O\" ma siamo all'interno di una serie di tag non \"O\", chiudi l'entità corrente\n",
    "            if current_entity_span is not None:\n",
    "                entity_spans.append(current_entity_span)\n",
    "                current_entity_span = None\n",
    "\n",
    "    # Aggiungi l'ultima entità se presente\n",
    "    if current_entity_span is not None:\n",
    "        entity_spans.append(current_entity_span)\n",
    "\n",
    "    # Creazione del dataframe di output\n",
    "    output_data = {'Tag': [], 'Sentence Number': [], 'Start Index': [], 'End Index': []}\n",
    "    for entity_span in entity_spans:\n",
    "        output_data['Tag'].append(entity_span['Tag'])\n",
    "        output_data['Sentence Number'].append(entity_span['Sentence Number'])\n",
    "        output_data['Start Index'].append(entity_span['Start Index'])\n",
    "        output_data['End Index'].append(entity_span['End Index'])\n",
    "\n",
    "    output_df = pd.DataFrame(output_data)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_accuracy(system_df, golden_df):\n",
    "    # Uniamo i due dataframe per confrontare i tag\n",
    "    merged_df = pd.merge(system_df, golden_df, left_index=True, right_index=True, suffixes=('_system', '_golden'))\n",
    "    \n",
    "    # Contiamo quante volte i tag corrispondenti sono uguali\n",
    "    correct_tags = (merged_df['TAG_system'] == merged_df['TAG_golden']).sum()\n",
    "    \n",
    "    # Calcoliamo l'accuratezza\n",
    "    accuracy = correct_tags / len(system_df)\n",
    "    \n",
    "    # Converti in percentuale e arrotonda alla prima cifra decimale\n",
    "    accuracy_percent = round(accuracy * 100, 1)\n",
    "    \n",
    "    return accuracy_percent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_precision_recall(predicted_df, golden_df,entity_type=\"all\"):\n",
    "    # Unione delle entità predette e delle entità del sistema dorato\n",
    "    merged_df = pd.merge(predicted_df, golden_df, how='outer', indicator=True)\n",
    "    if entity_type!=\"all\":\n",
    "        merged_df=merged_df[(merged_df['Tag'] == entity_type)]\n",
    "    # Calcolo dei true positives (TP), false positives (FP) e false negatives (FN)\n",
    "    TP = merged_df[(merged_df['_merge'] == 'both')].shape[0]\n",
    "    FP = merged_df[(merged_df['_merge'] == 'right_only')].shape[0]\n",
    "    FN = merged_df[(merged_df['_merge'] == 'left_only')].shape[0]\n",
    "\n",
    "    # Calcolo della precisione e del recall\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    # Converti in percentuali e arrotonda alla prima cifra decimale\n",
    "    precision_percent = round(precision * 100, 1)\n",
    "    recall_percent = round(recall * 100, 1)\n",
    "\n",
    "    return precision_percent, recall_percent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "#Scrive su file i risultati delle metriche di tutti gli algorithmi per tutte le lingue\n",
    "def evaluate_and_save_results(languages, output_file, dataset_size, smoothing_type):\n",
    "    sep = \"-\" * 139 + \"\\n\" \n",
    "    sep2= \"#\"*139 + \"\\n\"\n",
    "    header_format = \"{:<4} | {:<7} | {:<5} | {:<5} | {:<5} | {:<10} | {:<8} | {:<7} | {:<8} | {:<7} | {:<8} | {:<7} | {:<8} | {:<7} |\\n\"\n",
    "    row_format = \"{:<4} | {:<7} | {:<5.1f} | {:<5.1f} | {:<5.1f} | {:<10} | {:<8.1f} | {:<7.1f} | {:<8.1f} | {:<7.1f} | {:<8.1f} | {:<7.1f} | {:<9.1f} | {:<8.1f} |\\n\"\n",
    "\n",
    "   \n",
    "    with open(output_file, \"a\") as file:\n",
    "        file.write(sep2)\n",
    "        file.write(f\"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        file.write(f\"Dimensione del dataset: {dataset_size}\\n\")\n",
    "        file.write(f\"Tipo di Smoothing: {'Smoothing Type ' + str(smoothing_type)}\\n\")\n",
    "        file.write(sep)\n",
    "        file.write(\"Risultati:\\n\")\n",
    "        file.write(header_format.format(\"Lang\", \"Alg\", \"Acc\", \"Prec\", \"Rec\", \"\", \"LOC_Prec\", \"LOC_Rec\", \"PER_Prec\", \"PER_Rec\", \"ORG_Prec\", \"ORG_Rec\", \"MISC_Prec\", \"MISC_Rec\"))\n",
    "        \n",
    "        file.write(sep)\n",
    "        for language in languages:\n",
    "            gold_df = read_tagging(\"golden_tag\", language)\n",
    "            gold_quadruples_df = extract_entities_from_dataframe(gold_df)\n",
    "            for alg in [\"naive\",\"viterbi\"]:\n",
    "                result_df = read_tagging(str(alg)+\"_tag\", language)\n",
    "                result_quadruples = extract_entities_from_dataframe(result_df)\n",
    "                metrics=[]\n",
    "                metrics.append(calculate_accuracy(result_df, gold_df))\n",
    "                for entity in [\"all\",\"LOC\",\"PER\",\"ORG\",\"MISC\"]:\n",
    "                    precision, recall = calculate_precision_recall(result_quadruples, gold_quadruples_df,entity)\n",
    "                    metrics.append(precision)\n",
    "                    metrics.append(recall)\n",
    "            \n",
    "                file.write(row_format.format(language, alg, metrics[0], metrics[1], metrics[2], \"\", metrics[3], metrics[4], metrics[5], metrics[6], metrics[7], metrics[8], metrics[9], metrics[10]))\n",
    "            file.write(sep)\n",
    "        file.write(sep2+\"\\n\")\n",
    "\n",
    "# Esempio di utilizzo\n",
    "languages = [\"en\", \"it\", \"es\"]\n",
    "output_file = \"../evaluation_results.txt\"\n",
    "dataset_size = 100\n",
    "smoothing_type = 4\n",
    "evaluate_and_save_results(languages, output_file, dataset_size, smoothing_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "#VERSIONE VECCHIA\n",
    "# def evaluate_and_save_results(languages, output_file, dataset_size, smoothing_type):\n",
    "   \n",
    "#     with open(output_file, \"a\") as file:\n",
    "#         file.write(\"##################################\\n\")\n",
    "#         file.write(f\"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "#         file.write(f\"Dimensione del dataset: {dataset_size}\\n\")\n",
    "#         file.write(f\"Tipo di Smoothing: {'Smoothing Type ' + str(smoothing_type)}\\n\\n\")\n",
    "#         file.write(\"Statistiche aggiuntive:\\n\")\n",
    "#         file.write(\"Language ||| Acc Naive | Precision Naive | Recall Naive ||| Acc Viterbi| Precision Viterbi| Recall Viterbi\\n\")\n",
    "        \n",
    "#         for language in languages:\n",
    "#             vit_df= read_tagging(\"viterbi_tag\", language)\n",
    "#             naive_df = read_tagging(\"naive_tag\", language)\n",
    "#             golden_df = read_tagging(\"golden_tag\", language)\n",
    "            \n",
    "#             gold_quadruples_df = extract_entities_from_dataframe(golden_df)\n",
    "#             naive_quadruples_df = extract_entities_from_dataframe(naive_df)\n",
    "#             vit_quadruples_df = extract_entities_from_dataframe(vit_df)\n",
    "            \n",
    "\n",
    "         \n",
    "#             acc_vit= calculate_accuracy(vit_df, golden_df)\n",
    "#             acc_naive = calculate_accuracy(naive_df, golden_df)\n",
    "            \n",
    "#             precision_vit, recall_vit = calculate_precision_recall(vit_quadruples_df, gold_quadruples_df)\n",
    "#             precision_n, recall_n = calculate_precision_recall(naive_quadruples_df, gold_quadruples_df)\n",
    "#             file.write(f\"{language} ||| {format(acc_naive, '.1f')} % | {format(precision_n, '.1f')} % | {format(recall_n, '.1f')} % ||| {format(acc_vit, '.1f')} % | {format(precision_vit, '.1f')} % | {format(recall_vit, '.1f')} % |\\n\")\n",
    "        \n",
    "#         file.write(\"##################################\\n \\n\")\n",
    "\n",
    "# # Esempio di utilizzo\n",
    "# languages = [\"en\", \"it\", \"es\"]\n",
    "# output_file = \"../evaluation_results.txt\"\n",
    "# dataset_size = 100000\n",
    "# smoothing_type = 4\n",
    "# evaluate_and_save_results(languages, output_file, dataset_size, smoothing_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
