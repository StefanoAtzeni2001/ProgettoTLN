{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# function to read data from file\n",
    "def read_tagging(file_name,language):\n",
    "    path=\"../data/\"+language+\"/tagging/\"+file_name+\".conllu\"\n",
    "    data = pd.read_csv (path, sep = '\\t',quoting=3, names=[\"POSITION\",\"WORD\",\"TAG\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_sentences_from_dataframe(df):\n",
    "    sentences = ''\n",
    "    for index, row in df.iterrows():\n",
    "        word = row['WORD']\n",
    "        if pd.notnull(word):  # Se la parola non è nulla\n",
    "            if sentences:  # Se c'è già una frase, aggiungi uno spazio prima della nuova parola\n",
    "                sentences += ' '\n",
    "            sentences += word\n",
    "    return sentences\n",
    "\n",
    "#deserialize data from a file\n",
    "def load_data(file_name,language):\n",
    "    path=\"../data/\"+language+\"/\"+file_name\n",
    "    try: \n",
    "        file = open(path, 'rb') \n",
    "        data = pickle.load(file) \n",
    "        return data\n",
    "    except: \n",
    "        print(\"Error in reading data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_entities_from_dataframe(dataframe):\n",
    "    entity_spans = []\n",
    "\n",
    "    current_entity_span = None\n",
    "    current_sentence_index = 0\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        word = row['WORD']\n",
    "        tag = row['TAG']\n",
    "        position = row['POSITION']\n",
    "\n",
    "        if position == 0:  # Inizio di una nuova frase\n",
    "            current_sentence_index += 1\n",
    "\n",
    "        if tag != 'O':\n",
    "            if tag.startswith('B-'):\n",
    "                # Se inizia una nuova entità, chiudi quella corrente e inizia una nuova\n",
    "                if current_entity_span is not None:\n",
    "                    entity_spans.append(current_entity_span)\n",
    "                current_entity_span = {'Tag': tag[2:], 'Sentence Number': current_sentence_index}\n",
    "                current_entity_span['Start Index'] = index\n",
    "                current_entity_span['End Index'] = index\n",
    "            elif tag.startswith('I-'):\n",
    "                # Aggiungi la parola all'entità corrente\n",
    "                if current_entity_span is None:\n",
    "                    print()\n",
    "                else:\n",
    "                    current_entity_span['End Index'] = index\n",
    "            else:\n",
    "                print(\"Errore: Tag non riconosciuto.\")\n",
    "\n",
    "        else:\n",
    "            # Se il tag è \"O\" ma siamo all'interno di una serie di tag non \"O\", chiudi l'entità corrente\n",
    "            if current_entity_span is not None:\n",
    "                entity_spans.append(current_entity_span)\n",
    "                current_entity_span = None\n",
    "\n",
    "    # Aggiungi l'ultima entità se presente\n",
    "    if current_entity_span is not None:\n",
    "        entity_spans.append(current_entity_span)\n",
    "\n",
    "    # Creazione del dataframe di output\n",
    "    output_data = {'Tag': [], 'Sentence Number': [], 'Start Index': [], 'End Index': []}\n",
    "    for entity_span in entity_spans:\n",
    "        output_data['Tag'].append(entity_span['Tag'])\n",
    "        output_data['Sentence Number'].append(entity_span['Sentence Number'])\n",
    "        output_data['Start Index'].append(entity_span['Start Index'])\n",
    "        output_data['End Index'].append(entity_span['End Index'])\n",
    "\n",
    "    output_df = pd.DataFrame(output_data)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_accuracy(system_df, golden_df):\n",
    "    # Uniamo i due dataframe per confrontare i tag\n",
    "    merged_df = pd.merge(system_df, golden_df, left_index=True, right_index=True, suffixes=('_system', '_golden'))\n",
    "    \n",
    "    # Contiamo quante volte i tag corrispondenti sono uguali\n",
    "    correct_tags = (merged_df['TAG_system'] == merged_df['TAG_golden']).sum()\n",
    "    \n",
    "    # Calcoliamo l'accuratezza\n",
    "    accuracy = correct_tags / len(system_df)\n",
    "    \n",
    "    # Converti in percentuale e arrotonda alla prima cifra decimale\n",
    "    accuracy_percent = round(accuracy * 100, 1)\n",
    "    \n",
    "    return accuracy_percent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_precision_recall(predicted_df, golden_df):\n",
    "    # Unione delle entità predette e delle entità del sistema dorato\n",
    "    merged_df = pd.merge(predicted_df, golden_df, how='outer', indicator=True)\n",
    "\n",
    "    # Calcolo dei true positives (TP), false positives (FP) e false negatives (FN)\n",
    "    TP = merged_df[(merged_df['_merge'] == 'both')].shape[0]\n",
    "    FP = merged_df[(merged_df['_merge'] == 'right_only')].shape[0]\n",
    "    FN = merged_df[(merged_df['_merge'] == 'left_only')].shape[0]\n",
    "\n",
    "    # Calcolo della precisione e del recall\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    # Converti in percentuali e arrotonda alla prima cifra decimale\n",
    "    precision_percent = round(precision * 100, 1)\n",
    "    recall_percent = round(recall * 100, 1)\n",
    "\n",
    "    return precision_percent, recall_percent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m dataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     43\u001b[0m smoothing_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m---> 44\u001b[0m \u001b[43mevaluate_and_save_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m, in \u001b[0;36mevaluate_and_save_results\u001b[1;34m(languages, output_file, dataset_size, smoothing_type)\u001b[0m\n\u001b[0;32m     13\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatistiche aggiuntive:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanguage | Acc Viterbi | Precision Viterbi | Recall Viterbi | Acc Nayve | Precision Nayve | Recall Nayve | Acc Viterbi Sent | Precision Viterbi Sent | Recall Viterbi Sent\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m language \u001b[38;5;129;01min\u001b[39;00m languages:\n\u001b[0;32m     17\u001b[0m     tot_vit_df \u001b[38;5;241m=\u001b[39m read_tagging(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviterbi_tag\u001b[39m\u001b[38;5;124m\"\u001b[39m, language)\n\u001b[0;32m     18\u001b[0m     sent_vit_df\u001b[38;5;241m=\u001b[39m read_tagging(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_viterbi_tag\u001b[39m\u001b[38;5;124m\"\u001b[39m, language)\n",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m, in \u001b[0;36mevaluate_and_save_results\u001b[1;34m(languages, output_file, dataset_size, smoothing_type)\u001b[0m\n\u001b[0;32m     13\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatistiche aggiuntive:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanguage | Acc Viterbi | Precision Viterbi | Recall Viterbi | Acc Nayve | Precision Nayve | Recall Nayve | Acc Viterbi Sent | Precision Viterbi Sent | Recall Viterbi Sent\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m language \u001b[38;5;129;01min\u001b[39;00m languages:\n\u001b[0;32m     17\u001b[0m     tot_vit_df \u001b[38;5;241m=\u001b[39m read_tagging(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviterbi_tag\u001b[39m\u001b[38;5;124m\"\u001b[39m, language)\n\u001b[0;32m     18\u001b[0m     sent_vit_df\u001b[38;5;241m=\u001b[39m read_tagging(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_viterbi_tag\u001b[39m\u001b[38;5;124m\"\u001b[39m, language)\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def evaluate_and_save_results(languages, output_file, dataset_size, smoothing_type):\n",
    "\n",
    "    # Scrivi i risultati su file\n",
    "   \n",
    "    with open(output_file, \"a\") as file:\n",
    "        file.write(\"##################################\\n\")\n",
    "        file.write(f\"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        file.write(f\"Dimensione del dataset: {dataset_size}\\n\")\n",
    "        file.write(f\"Tipo di Smoothing: {'Smoothing Type ' + str(smoothing_type)}\\n\\n\")\n",
    "        file.write(\"Statistiche aggiuntive:\\n\")\n",
    "        file.write(\"Language | Acc Viterbi | Precision Viterbi | Recall Viterbi | Acc Nayve | Precision Nayve | Recall Nayve | Acc Viterbi Sent | Precision Viterbi Sent | Recall Viterbi Sent\\n\")\n",
    "        \n",
    "        for language in languages:\n",
    "            tot_vit_df = read_tagging(\"viterbi_tag\", language)\n",
    "            sent_vit_df= read_tagging(\"sent_viterbi_tag\", language)\n",
    "            nayve_df = read_tagging(\"nayve_tag\", language)\n",
    "            golden_df = read_tagging(\"golden_tag\", language)\n",
    "            \n",
    "            gol_quadruples_df = extract_entities_from_dataframe(golden_df)\n",
    "            nay_quadruples_df = extract_entities_from_dataframe(nayve_df)\n",
    "            tot_vit_quadruples_df = extract_entities_from_dataframe(tot_vit_df)\n",
    "            sent_vit_quadruples_df = extract_entities_from_dataframe(sent_vit_df)\n",
    "            \n",
    "\n",
    "            acc_vit = calculate_accuracy(tot_vit_df, golden_df)\n",
    "            acc_vit_sent = calculate_accuracy(sent_vit_df, golden_df)\n",
    "            acc_nayve = calculate_accuracy(nayve_df, golden_df)\n",
    "            precision_v, recall_v = calculate_precision_recall(tot_vit_quadruples_df, gol_quadruples_df)\n",
    "            precision_v_sent, recall_v_sent = calculate_precision_recall(sent_vit_quadruples_df, gol_quadruples_df)\n",
    "            precision_n, recall_n = calculate_precision_recall(nay_quadruples_df, gol_quadruples_df)\n",
    "\n",
    "            file.write(f\"{language} || {format(acc_vit, '.1f')} % | {format(precision_v, '.1f')} % | {format(recall_v, '.1f')} % ||| {format(acc_nayve, '.1f')} % | {format(precision_n, '.1f')} % | {format(recall_n, '.1f')} % ||| {format(acc_vit_sent, '.1f')} % | {format(precision_v_sent, '.1f')} % | {format(recall_v_sent, '.1f')} % |\\n\")\n",
    "        \n",
    "        file.write(\"##################################\\n \\n\")\n",
    "\n",
    "# Esempio di utilizzo\n",
    "languages = [\"en\", \"it\", \"es\"]\n",
    "output_file = \"../evaluation_results.txt\"\n",
    "dataset_size = 1000\n",
    "smoothing_type = 4\n",
    "evaluate_and_save_results(languages, output_file, dataset_size, smoothing_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
