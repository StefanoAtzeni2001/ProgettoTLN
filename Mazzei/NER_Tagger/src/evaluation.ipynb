{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# function to read data from file\n",
    "def read_tagging(file_name,language):\n",
    "    path=\"../data/\"+language+\"/tagging/\"+file_name+\".conllu\"\n",
    "    data = pd.read_csv (path, sep = '\\t',quoting=3, names=[\"POSITION\",\"WORD\",\"TAG\"])\n",
    "    return data\n",
    "\n",
    "def extract_sentences_from_dataframe(df):\n",
    "    sentences = ''\n",
    "    for index, row in df.iterrows():\n",
    "        word = row['WORD']\n",
    "        if pd.notnull(word):  # Se la parola non è nulla\n",
    "            if sentences:  # Se c'è già una frase, aggiungi uno spazio prima della nuova parola\n",
    "                sentences += ' '\n",
    "            sentences += word\n",
    "    return sentences\n",
    "\n",
    "#deserialize data from a file\n",
    "def load_data(file_name,language):\n",
    "    path=\"../data/\"+language+\"/\"+file_name\n",
    "    try: \n",
    "        file = open(path, 'rb') \n",
    "        data = pickle.load(file) \n",
    "        return data\n",
    "    except: \n",
    "        print(\"Error in reading data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversione di un dataframe che contine le frasi etichettate \n",
    "#in un dataframe che contiene solo le entità etichettate nella forma di quadrupe (tag, sentence number, start index, end index)\n",
    "def extract_entities_from_dataframe(dataframe):\n",
    "    entity_spans = []\n",
    "\n",
    "    current_entity_span = None\n",
    "    current_sentence_index = 0\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        word = row['WORD']\n",
    "        tag = row['TAG']\n",
    "        position = row['POSITION']\n",
    "\n",
    "        if position == 0:  # Inizio di una nuova frase\n",
    "            current_sentence_index += 1\n",
    "\n",
    "        if tag != 'O':\n",
    "            if tag.startswith('B-'):\n",
    "                # Se inizia una nuova entità, chiudi quella corrente e inizia una nuova\n",
    "                if current_entity_span is not None:\n",
    "                    entity_spans.append(current_entity_span)\n",
    "                current_entity_span = {'Tag': tag[2:], 'Sentence Number': current_sentence_index}\n",
    "                current_entity_span['Start Index'] = index\n",
    "                current_entity_span['End Index'] = index\n",
    "            elif tag.startswith('I-'):\n",
    "                # Aggiungi la parola all'entità corrente\n",
    "                if current_entity_span is not None:\n",
    "                    current_entity_span['End Index'] = index\n",
    "            else:\n",
    "                print(\"Errore: Tag non riconosciuto.\")\n",
    "\n",
    "        else:\n",
    "            # Se il tag è \"O\" ma siamo all'interno di una serie di tag non \"O\", chiudi l'entità corrente\n",
    "            if current_entity_span is not None:\n",
    "                entity_spans.append(current_entity_span)\n",
    "                current_entity_span = None\n",
    "\n",
    "    # Aggiungo l'ultima entità se presente\n",
    "    if current_entity_span is not None:\n",
    "        entity_spans.append(current_entity_span)\n",
    "\n",
    "    # Creazione del dataframe di output\n",
    "    output_data = {'Tag': [], 'Sentence Number': [], 'Start Index': [], 'End Index': []}\n",
    "    for entity_span in entity_spans:\n",
    "        output_data['Tag'].append(entity_span['Tag'])\n",
    "        output_data['Sentence Number'].append(entity_span['Sentence Number'])\n",
    "        output_data['Start Index'].append(entity_span['Start Index'])\n",
    "        output_data['End Index'].append(entity_span['End Index'])\n",
    "\n",
    "    output_df = pd.DataFrame(output_data)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(system_df, golden_df):\n",
    "    # Unione i due dataframe per confrontare i tag\n",
    "    merged_df = pd.merge(system_df, golden_df, left_index=True, right_index=True, suffixes=('_system', '_golden'))\n",
    "    # Conteggio di tag corrispondenti uguali\n",
    "    correct_tags = (merged_df['TAG_system'] == merged_df['TAG_golden']).sum()\n",
    "    accuracy = correct_tags / len(system_df)\n",
    "    accuracy_percent = round(accuracy * 100, 1)\n",
    "    return accuracy_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(predicted_df, golden_df,entity_type=\"all\"):\n",
    "    # Unione delle entità predette e delle entità corrette\n",
    "    merged_df = pd.merge(predicted_df, golden_df, how='outer', indicator=True)\n",
    "    # Filtra per tipo di entità se specificato\n",
    "    if entity_type!=\"all\":\n",
    "        merged_df=merged_df[(merged_df['Tag'] == entity_type)]\n",
    "    # Calcolo dei true positives (TP), false positives (FP) e false negatives (FN)\n",
    "    TP = merged_df[(merged_df['_merge'] == 'both')].shape[0]\n",
    "    FP = merged_df[(merged_df['_merge'] == 'right_only')].shape[0]\n",
    "    FN = merged_df[(merged_df['_merge'] == 'left_only')].shape[0]\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    precision_percent = round(precision * 100, 1)\n",
    "    recall_percent = round(recall * 100, 1)\n",
    "\n",
    "    return precision_percent, recall_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "#Scrive su file i risultati delle metriche di tutti gli algorithmi per tutte le lingue\n",
    "def evaluate_and_save_results(languages, output_file, dataset_size):\n",
    "    sep = \"-\" * 139 + \"\\n\" \n",
    "    sep2= \"#\"*139 + \"\\n\"\n",
    "    header_format = \"{:<4} | {:<7} | {:<5} | {:<5} | {:<5} | {:<10} | {:<8} | {:<7} | {:<8} | {:<7} | {:<8} | {:<7} | {:<8} | {:<7} |\\n\"\n",
    "    row_format = \"{:<4} | {:<7} | {:<5.1f} | {:<5.1f} | {:<5.1f} | {:<10} | {:<8.1f} | {:<7.1f} | {:<8.1f} | {:<7.1f} | {:<8.1f} | {:<7.1f} | {:<9.1f} | {:<8.1f} |\\n\"\n",
    "\n",
    "   \n",
    "    with open(output_file, \"a\") as file:\n",
    "        file.write(sep2)\n",
    "        file.write(f\"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        file.write(f\"Dimensione del dataset: {dataset_size}\\n\")\n",
    "        file.write(f\"Tipo di Smoothing: \"+ open('../data/it/tagging/smoothing_type', 'r').read()+\"\\n\")\n",
    "        file.write(sep)\n",
    "        file.write(\"Risultati:\\n\")\n",
    "        file.write(header_format.format(\"Lang\", \"Alg\", \"Acc\", \"Prec\", \"Rec\", \"\", \"LOC_Prec\", \"LOC_Rec\", \"PER_Prec\", \"PER_Rec\", \"ORG_Prec\", \"ORG_Rec\", \"MISC_Prec\", \"MISC_Rec\"))\n",
    "        \n",
    "        file.write(sep)\n",
    "        for language in languages:\n",
    "            gold_df = read_tagging(\"golden_tag\", language)[:dataset_size]\n",
    "            gold_quadruples_df = extract_entities_from_dataframe(gold_df)\n",
    "            for alg in [\"naive\",\"viterbi\",\"memm\"]:\n",
    "                result_df = read_tagging(str(alg)+\"_tag\", language)[:dataset_size]\n",
    "                result_quadruples = extract_entities_from_dataframe(result_df)\n",
    "                metrics=[]\n",
    "                metrics.append(calculate_accuracy(result_df, gold_df))\n",
    "                for entity in [\"all\",\"LOC\",\"PER\",\"ORG\",\"MISC\"]:\n",
    "                    precision, recall = calculate_precision_recall(result_quadruples, gold_quadruples_df,entity)\n",
    "                    metrics.append(precision)\n",
    "                    metrics.append(recall)\n",
    "            \n",
    "                file.write(row_format.format(language, alg, metrics[0], metrics[1], metrics[2], \"\", metrics[3], metrics[4], metrics[5], metrics[6], metrics[7], metrics[8], metrics[9], metrics[10]))\n",
    "            file.write(sep)\n",
    "        file.write(sep2+\"\\n\")\n",
    "\n",
    "# Esempio di utilizzo\n",
    "languages = [\"en\"]#, \"it\", \"es\"]\n",
    "output_file = \"../evaluation_results.txt\"\n",
    "dataset_size = 2200 #numero di righe (token) da considerare\n",
    "\n",
    "evaluate_and_save_results(languages, output_file, dataset_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
